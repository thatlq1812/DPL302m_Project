{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thatlq1812/DPL302m_Project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aPDB_w2It-U"
      },
      "source": [
        "wait for fill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y2rq_97m9tFo"
      },
      "outputs": [],
      "source": [
        "# Google Colab library\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    import torch_xla.core.xla_model as xm\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJP8EPTiIt-W",
        "outputId": "d60a5ef4-f670-4bc3-9a03-ae5b812216c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "# Python library\n",
        "import os\n",
        "import zipfile\n",
        "import sys\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import multiprocessing\n",
        "import pickle as pkl\n",
        "import yaml\n",
        "import albumentations as A\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Sklearn library\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pytorch library\n",
        "import torch\n",
        "\n",
        "from torch import optim, nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import models,transforms\n",
        "\n",
        "# CV2 library\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUleo0yj9G6R",
        "outputId": "789d960c-8337-4357-bbab-62e0ced0a526"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    if not os.path.exists(\"/content/data\"):\n",
        "        !cp \"/content/drive/My Drive/Colab Notebooks/data.zip\" \"/content/\"\n",
        "        with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"/content/\")\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnP11fOXIt-W",
        "outputId": "bccaf6ba-05af-475b-cf08-1c0bfc23db9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project name:  Cancer Detection\n",
            "Project version:  DPL302m Project - Fall 2024 of Group 1\n",
            "Project author:  Group 1\n",
            "Project version:  1.0.0\n",
            "Data path:  {'data_path': 'data/', 'csv_path': 'data/HAM10000_metadata.csv', 'images_path': 'data/images/'}\n",
            "N workers:  16\n",
            "Batch size:  32\n",
            "Preprocessing configuration\n",
            "Augmentation ratios:  1\n",
            "Resize shape:  (224, 224)\n"
          ]
        }
      ],
      "source": [
        "# Load configuration\n",
        "with open('config.yaml') as f:\n",
        "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "# Project information\n",
        "print('Project name: ', config['project']['name'])\n",
        "print('Project version: ', config['project']['description'])\n",
        "print('Project author: ', config['project']['author'])\n",
        "print('Project version: ', config['project']['version'])\n",
        "\n",
        "# Data location\n",
        "print('Data path: ', config['data'])\n",
        "data_path = config['data']['data_path']\n",
        "csv_path = config['data']['csv_path']\n",
        "images_path = config['data']['images_path']\n",
        "\n",
        "# Training configuration\n",
        "n_workers = multiprocessing.cpu_count()\n",
        "batch_size = config['training']['batch_size']\n",
        "print('N workers: ', n_workers)\n",
        "print('Batch size: ', batch_size)\n",
        "\n",
        "# Preprocessing configuration\n",
        "print('Preprocessing configuration')\n",
        "augmentation_ratio = config['preprocessing']['augmentation']['ratio']\n",
        "resize_shape = tuple(config['preprocessing']['resize'])\n",
        "print('Augmentation ratios: ', augmentation_ratio)\n",
        "print('Resize shape: ', resize_shape)\n",
        "\n",
        "# Create a dictionary for images location\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'dermatofibroma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lwNyHy6RIt-X"
      },
      "outputs": [],
      "source": [
        "# Data reading\n",
        "data = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fB6lO8XeIt-X"
      },
      "outputs": [],
      "source": [
        "def data_check(data):\n",
        "    # Show data types\n",
        "    print('Data types:')\n",
        "    print(data.dtypes)\n",
        "    print()\n",
        "\n",
        "    # Show unique values for each column (skip image_id and lesion_id)\n",
        "    print('Unique values for each column:')\n",
        "    for col in data.columns:\n",
        "        if col in ['image_id', 'lesion_id']:\n",
        "            continue\n",
        "        print(f\"{col}: {data[col].unique()[:5]}...\")\n",
        "    print()\n",
        "\n",
        "    # Count NaN values\n",
        "    print('NaN values in each column:')\n",
        "    nan_counts = data.isnull().sum()\n",
        "    print(nan_counts[nan_counts > 0])\n",
        "    print()\n",
        "\n",
        "    # Count 'unknown' values (only for object columns)\n",
        "    print('Unknown values in each column:')\n",
        "    for col in data.select_dtypes(include=['object']).columns:\n",
        "        unknown_count = data[data[col] == 'unknown'].shape[0]\n",
        "        if unknown_count > 0:\n",
        "            print(f\"{col}: {unknown_count}\")\n",
        "    print()\n",
        "\n",
        "    # Summary\n",
        "    print(\"Summary of potential data issues:\")\n",
        "    print(f\"- Columns with NaN values: {nan_counts[nan_counts > 0].index.tolist()}\")\n",
        "    print(f\"- Columns with 'unknown' values: {[col for col in data.columns if 'unknown' in data[col].values]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo3T1ozRIt-X",
        "outputId": "20627c04-02f1-4882-a155-a7839f0a0e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data types:\n",
            "lesion_id        object\n",
            "image_id         object\n",
            "dx               object\n",
            "dx_type          object\n",
            "age             float64\n",
            "sex              object\n",
            "localization     object\n",
            "dtype: object\n",
            "\n",
            "Unique values for each column:\n",
            "dx: ['bkl' 'nv' 'df' 'mel' 'vasc']...\n",
            "dx_type: ['histo' 'consensus' 'confocal' 'follow_up']...\n",
            "age: [80. 75. 60. 70. 55.]...\n",
            "sex: ['male' 'female' 'unknown']...\n",
            "localization: ['scalp' 'ear' 'face' 'back' 'trunk']...\n",
            "\n",
            "NaN values in each column:\n",
            "age    57\n",
            "dtype: int64\n",
            "\n",
            "Unknown values in each column:\n",
            "sex: 57\n",
            "localization: 234\n",
            "\n",
            "Summary of potential data issues:\n",
            "- Columns with NaN values: ['age']\n",
            "- Columns with 'unknown' values: ['sex', 'localization']\n"
          ]
        }
      ],
      "source": [
        "data_check(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "41QqtJJqIt-X"
      },
      "outputs": [],
      "source": [
        "def data_cleaning(data):\n",
        "    # Drop duplicates defined by 'image_id' and 'lesion_id'\n",
        "    data = data.drop_duplicates(subset=['image_id'])\n",
        "\n",
        "    # Drop values with NaN\n",
        "    data = data.dropna(subset=['age', 'sex', 'localization'])\n",
        "    data = data[data['age'] != 'unknown']\n",
        "    data = data[data['sex'] != 'unknown']\n",
        "    data = data[data['localization'] != 'unknown']\n",
        "\n",
        "    # Convert 'age' to int\n",
        "    try:\n",
        "        data['age'] = data['age'].astype(int)\n",
        "    except ValueError:\n",
        "        print(\"Some 'age' values could not be converted to int.\")\n",
        "\n",
        "    # Create label encoders for each categorical column\n",
        "    label_encoder_dx = LabelEncoder()\n",
        "    label_encoder_dx_type = LabelEncoder()\n",
        "    label_encoder_sex = LabelEncoder()\n",
        "    label_encoder_localization = LabelEncoder()\n",
        "\n",
        "    # Encode labels\n",
        "    data['dx_code'] = label_encoder_dx.fit_transform(data['dx'])\n",
        "    data['dx_type_code'] = label_encoder_dx_type.fit_transform(data['dx_type'])\n",
        "    data['sex_code'] = label_encoder_sex.fit_transform(data['sex'])\n",
        "    data['localization_code'] = label_encoder_localization.fit_transform(data['localization'])\n",
        "\n",
        "    # Add location of images\n",
        "    data['image_id'] = data['image_id'].apply(lambda x: os.path.join(images_path, x + '.jpg'))\n",
        "\n",
        "    print('Data after cleaning:')\n",
        "    print(data.head())\n",
        "\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoMKvVjRIt-Y",
        "outputId": "6c944edc-5688-4947-ed27-f986edba31b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data after cleaning:\n",
            "     lesion_id                      image_id   dx dx_type  age   sex  \\\n",
            "0  HAM_0000118  data/images/ISIC_0027419.jpg  bkl   histo   80  male   \n",
            "1  HAM_0000118  data/images/ISIC_0025030.jpg  bkl   histo   80  male   \n",
            "2  HAM_0002730  data/images/ISIC_0026769.jpg  bkl   histo   80  male   \n",
            "3  HAM_0002730  data/images/ISIC_0025661.jpg  bkl   histo   80  male   \n",
            "4  HAM_0001466  data/images/ISIC_0031633.jpg  bkl   histo   75  male   \n",
            "\n",
            "  localization  dx_code  dx_type_code  sex_code  localization_code  \n",
            "0        scalp        2             3         1                 11  \n",
            "1        scalp        2             3         1                 11  \n",
            "2        scalp        2             3         1                 11  \n",
            "3        scalp        2             3         1                 11  \n",
            "4          ear        2             3         1                  4  \n",
            "Data types:\n",
            "lesion_id            object\n",
            "image_id             object\n",
            "dx                   object\n",
            "dx_type              object\n",
            "age                   int64\n",
            "sex                  object\n",
            "localization         object\n",
            "dx_code               int64\n",
            "dx_type_code          int64\n",
            "sex_code              int64\n",
            "localization_code     int64\n",
            "dtype: object\n",
            "\n",
            "Unique values for each column:\n",
            "dx: ['bkl' 'nv' 'df' 'mel' 'vasc']...\n",
            "dx_type: ['histo' 'consensus' 'confocal' 'follow_up']...\n",
            "age: [80 75 60 70 55]...\n",
            "sex: ['male' 'female']...\n",
            "localization: ['scalp' 'ear' 'face' 'back' 'trunk']...\n",
            "dx_code: [2 5 3 4 6]...\n",
            "dx_type_code: [3 1 0 2]...\n",
            "sex_code: [1 0]...\n",
            "localization_code: [11  4  5  2 12]...\n",
            "\n",
            "NaN values in each column:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Unknown values in each column:\n",
            "\n",
            "Summary of potential data issues:\n",
            "- Columns with NaN values: []\n",
            "- Columns with 'unknown' values: []\n"
          ]
        }
      ],
      "source": [
        "data = data_cleaning(data)\n",
        "data_check(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0LQUopF6It-Y"
      },
      "outputs": [],
      "source": [
        "def split_data(data, test_size=0.2, random_state=42):\n",
        "    # Split data into training and testing sets\n",
        "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
        "    print(\"Train data shape:\", train_data.shape)\n",
        "    print(\"Test data shape:\", test_data.shape)\n",
        "\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8_HM7_NIt-Y",
        "outputId": "c32bca0f-82f5-48cc-9349-b1167a65eaec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (7808, 11)\n",
            "Test data shape: (1953, 11)\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = split_data(data, config['training']['test_size'], config['training']['random_state'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mqPeFLqkIt-Y"
      },
      "outputs": [],
      "source": [
        "def load_images_worker(batch):\n",
        "    images = []\n",
        "    for index, row in batch.iterrows():\n",
        "        image_path = row['image_id']\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print('Image not found: ', image_path)\n",
        "            continue\n",
        "        # Preprocess image\n",
        "        image = cv2.resize(image, resize_shape)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "def load_images(data, batch_size, num_workers):\n",
        "    all_images = []\n",
        "    batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
        "    st = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = {executor.submit(load_images_worker, batch): batch for batch in batches}\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "            img_batch = future.result()\n",
        "            if img_batch is not None:\n",
        "                all_images.extend(img_batch)\n",
        "    et = time.time()\n",
        "    print('Loaded {} images in {} seconds'.format(len(all_images), et - st))\n",
        "    return all_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlw57-7vIt-Y",
        "outputId": "2c2c3b3f-9d51-428a-daaa-674eaee73e36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 306/306 [00:11<00:00, 27.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 9761 images in 11.350366592407227 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 244/244 [00:06<00:00, 40.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 7808 images in 6.082642316818237 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 62/62 [00:01<00:00, 36.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1953 images in 1.7812118530273438 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load train images\n",
        "images = load_images(data, batch_size=batch_size, num_workers=n_workers)\n",
        "train_images = load_images(train_data, batch_size=batch_size, num_workers=n_workers)\n",
        "test_images = load_images(test_data, batch_size=batch_size, num_workers=n_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SM35KnpZIt-Y"
      },
      "outputs": [],
      "source": [
        "def type_count(data, rate):\n",
        "    lesion_type = data.groupby(data['dx']).count()\n",
        "    lesion_type = lesion_type.reset_index()\n",
        "    lesion_type = lesion_type[['dx', 'image_id']]\n",
        "    lesion_type.columns = ['dx', 'count']\n",
        "    lesion_type = lesion_type.sort_values(by='count', ascending=False)\n",
        "    print(lesion_type)\n",
        "    max_count = lesion_type['count'].max()\n",
        "    augmentation_counts = {}\n",
        "    for index, row in lesion_type.iterrows():\n",
        "        augmentation_counts[row['dx']] = int(max_count / row['count'] * rate)\n",
        "    return augmentation_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IZfar_pIt-Z",
        "outputId": "7b688339-c783-44e4-d2b0-c1b53fee6b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data:\n",
            "      dx  count\n",
            "5     nv   5205\n",
            "4    mel    876\n",
            "2    bkl    849\n",
            "1    bcc    404\n",
            "0  akiec    265\n",
            "6   vasc    120\n",
            "3     df     89\n",
            "\n",
            "Test Data:\n",
            "      dx  count\n",
            "5     nv   1286\n",
            "2    bkl    227\n",
            "4    mel    225\n",
            "1    bcc    105\n",
            "0  akiec     62\n",
            "3     df     26\n",
            "6   vasc     22\n",
            "\n",
            "Augmentation Ratios: {'nv': 1, 'mel': 5, 'bkl': 6, 'bcc': 12, 'akiec': 19, 'vasc': 43, 'df': 58}\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Data:\")\n",
        "augmentation_counts = type_count(train_data, config['preprocessing']['augmentation']['ratio'])\n",
        "print(\"\\nTest Data:\")\n",
        "_ = type_count(test_data, 0)\n",
        "print(\"\\nAugmentation Ratios:\", augmentation_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ZtYpTB5It-Z"
      },
      "outputs": [],
      "source": [
        "run_augmentation = True\n",
        "\n",
        "def augment_image_worker(row, image, augmentations, augmentation_counts):\n",
        "    augmented_images = []\n",
        "    augmented_data = []\n",
        "\n",
        "    lesion_type = row['dx']\n",
        "    count = augmentation_counts.get(lesion_type, 0)\n",
        "    if count == 0:\n",
        "        return None, None\n",
        "\n",
        "    for _ in range(count):\n",
        "        if _ == 0:\n",
        "            augmented_images.append(image)\n",
        "            augmented_row = row.copy()\n",
        "            augmented_row['image_id'] = row['image_id'] + '_original'\n",
        "            augmented_data.append(augmented_row)\n",
        "            continue\n",
        "\n",
        "        augment_image = augmentations(image=image)\n",
        "        augmented_images.append(augment_image['image'])\n",
        "        augmented_row = row.copy()\n",
        "        augmented_row['image_id'] = row['image_id'] + '_augmented_' + str(_)\n",
        "        augmented_data.append(augmented_row)\n",
        "\n",
        "    return augmented_data, augmented_images\n",
        "\n",
        "def augment_images(data, images_array, augmentation_counts, num_workers=n_workers):\n",
        "    augmentations = A.Compose([\n",
        "        A.HorizontalFlip(p=0.25),\n",
        "        A.VerticalFlip(p=0.25),\n",
        "        A.Rotate(limit=(0, 180), p=0.25),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    ])\n",
        "\n",
        "    print(f'Number of images: {len(images_array)}, Number of rows in data: {len(data)}')\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_data = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = []\n",
        "\n",
        "        for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Augmenting Images\"):\n",
        "            futures.append(\n",
        "                executor.submit(augment_image_worker, row, images_array[index], augmentations, augmentation_counts)\n",
        "            )\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Collecting Results\"):\n",
        "            aug_data, aug_images = future.result()\n",
        "            if aug_data is not None and aug_images is not None:\n",
        "                augmented_data.extend(aug_data)\n",
        "                augmented_images.extend(aug_images)\n",
        "\n",
        "    augmented_data = pd.DataFrame(augmented_data)\n",
        "\n",
        "    return augmented_data, augmented_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fky5S3QsIt-Z",
        "outputId": "68cd174d-1994-41a8-dc4f-63c4da5fbb10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 9761, Number of rows in data: 7808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Augmenting Images: 100%|██████████| 7808/7808 [00:01<00:00, 6903.10it/s]\n",
            "Collecting Results: 100%|██████████| 7808/7808 [00:14<00:00, 522.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape:  (7808, 11)\n",
            "Augmented data shape:  (34884, 11)\n",
            "Augmented images length:  34884\n"
          ]
        }
      ],
      "source": [
        "if run_augmentation:\n",
        "    augmented_train_data, augmented_train_images = augment_images(train_data, images, augmentation_counts)\n",
        "else:\n",
        "    augmented_train_data, augmented_train_images = train_data.copy(), images.copy()\n",
        "\n",
        "print('Train data shape: ', train_data.shape)\n",
        "print('Augmented data shape: ', augmented_train_data.shape)\n",
        "print('Augmented images length: ', len(augmented_train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCuvDLf99G6V",
        "outputId": "e49d7b9e-c288-4afc-b407-08436a1c4ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      dx  count\n",
            "5     nv   5205\n",
            "3     df   5162\n",
            "6   vasc   5160\n",
            "2    bkl   5094\n",
            "0  akiec   5035\n",
            "1    bcc   4848\n",
            "4    mel   4380\n"
          ]
        }
      ],
      "source": [
        "_ = type_count(augmented_train_data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sj3UOIcPIt-Z"
      },
      "outputs": [],
      "source": [
        "def compute_img_mean_std_worker(batch_images):\n",
        "    img_w, img_h = resize_shape\n",
        "    batch_imgs = []\n",
        "\n",
        "    for image in batch_images:\n",
        "        img = cv2.resize(image, (img_w, img_h))\n",
        "        batch_imgs.append(img)\n",
        "\n",
        "    return np.array(batch_imgs)\n",
        "\n",
        "def compute_img_mean_std(augmented_images, batch_size=32, num_workers=n_workers):\n",
        "    means = np.zeros(3)\n",
        "    stdevs = np.zeros(3)\n",
        "    total_count = 0\n",
        "\n",
        "    # Split images into batches\n",
        "    batches = [augmented_images[i:i + batch_size] for i in range(0, len(augmented_images), batch_size)]\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = {executor.submit(compute_img_mean_std_worker, batch): batch for batch in batches}\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Batches\"):\n",
        "            img_batch = future.result()\n",
        "            if img_batch is not None:\n",
        "                img_batch = img_batch.astype(np.float32) / 255.0  # Normalize\n",
        "\n",
        "                # Compute mean and std for each channel in the batch\n",
        "                for i in range(3):  # RGB channels\n",
        "                    means[i] += img_batch[:, :, :, i].mean() * img_batch.shape[0]\n",
        "                    stdevs[i] += (img_batch[:, :, :, i] ** 2).mean() * img_batch.shape[0]\n",
        "\n",
        "                total_count += img_batch.shape[0]  # Update total count\n",
        "\n",
        "    # Compute means and stds\n",
        "    means /= total_count\n",
        "    stdevs = np.sqrt(stdevs / total_count - means ** 2)\n",
        "\n",
        "    return means, stdevs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OFEahiIt-Z",
        "outputId": "2cf9975f-6164-40d2-8895-3168bf0a855f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images:  34884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Batches: 100%|██████████| 1091/1091 [00:34<00:00, 31.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normed means:  [0.75107293 0.56659155 0.59603678]\n",
            "Normed stds:  [0.15399802 0.16798469 0.18092112]\n"
          ]
        }
      ],
      "source": [
        "# Return the mean and std of the images\n",
        "print('Number of images: ', len(augmented_train_images))\n",
        "norm_mean, norm_std = compute_img_mean_std(augmented_train_images, batch_size=batch_size, num_workers=n_workers)\n",
        "print('Normed means: ', norm_mean)\n",
        "print('Normed stds: ', norm_std)\n",
        "\n",
        "# Save the mean and std into 1 file\n",
        "with open('norm.pkl', 'wb') as f:\n",
        "    pkl.dump({'mean': norm_mean, 'std': norm_std}, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3S4Ya9wtIt-a"
      },
      "outputs": [],
      "source": [
        "train_data_droped = augmented_train_data.drop(columns=['lesion_id', 'image_id', 'dx', 'dx_type','sex','localization'])\n",
        "train_data.head()\n",
        "train_data_droped.head()\n",
        "train_data_droped = train_data_droped.astype('float32')\n",
        "train_data_droped.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tALqWwDAIt-a"
      },
      "outputs": [],
      "source": [
        "# feature_extract is a boolean that defines if we are finetuning or feature extracting.\n",
        "# If feature_extract = False, the model is finetuned and all model parameters are updated.\n",
        "# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6--URsO0It-a"
      },
      "outputs": [],
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    # Variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18, resnet34, resnet50, resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = resize_shape[0]\n",
        "\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = resize_shape[0]\n",
        "\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet121\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = resize_shape[0]\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    return model_ft, input_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7p87QegIt-a",
        "outputId": "15d14e32-b0fe-46e3-8f44-c068bc5c9313"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# resnet, vgg, densenet, densenetcustom\n",
        "model_name = 'densenetcustom'\n",
        "num_classes = 7\n",
        "feature_extract = False\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "# Define the device:\n",
        "try:\n",
        "    device = xm.xla_device()\n",
        "except:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Put the model on the device:\n",
        "model = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SPF0Yw89It-a"
      },
      "outputs": [],
      "source": [
        "# define the transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KoaW4bu-It-a"
      },
      "outputs": [],
      "source": [
        "# \tage\tdx_code\tdx_type_code\tsex_code\tlocalization_code\n",
        "\n",
        "class HAM10000(Dataset):\n",
        "    def __init__(self, df, images_array, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_array = images_array  # Mảng chứa các ảnh\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get image name\n",
        "        X = self.images_array[index]\n",
        "        y = torch.tensor(int(self.df['dx_code'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        # return information\n",
        "        additional_info = {\n",
        "            'age': torch.tensor(int(self.df['age'][index])),\n",
        "            'dx_type': torch.tensor(int(self.df['dx_type_code'][index])),\n",
        "            'sex_code': torch.tensor(int(self.df['sex_code'][index])),\n",
        "            # 'localization': torch.tensor(int(self.df['localization_code'][index]))\n",
        "            # Change the additional_info_size in the CustomDenseNet121 model if you want to use 'localization'\n",
        "        }\n",
        "\n",
        "        return X, y, additional_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "M9f26tb4It-b"
      },
      "outputs": [],
      "source": [
        "# Define the training set using the table train_df and using our defined transitions (train_transform)\n",
        "training_set = HAM10000(train_data_droped,augmented_train_images,transform=transform)\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=n_workers)\n",
        "# Same for the validation set:\n",
        "validation_set = HAM10000(test_data,test_images, transform=transform)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=n_workers)\n",
        "\n",
        "\n",
        "# Lưu trữ dữ liệu để kiểm thử mà không cần load và xử lý lại ảnh, tên file có cả kích thước ảnh ( traindata64x64.pkl)\n",
        "with open('traindata'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.pkl', 'wb') as f:\n",
        "    pkl.dump({'train_data': train_data_droped, 'train_images': augmented_train_images, 'test_data': test_data, 'test_images': test_images}, f)\n",
        "\n",
        "with open('valdata'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.pkl', 'wb') as f:\n",
        "    pkl.dump({'train_data': train_data_droped, 'train_images': augmented_train_images, 'test_data': test_data, 'test_images': test_images}, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load lại dữ liệu\n",
        "train_loader = pkl.load(open('train_loader.pkl', 'rb'))\n",
        "val_loader = pkl.load(open('val_loader.pkl', 'rb'))\n",
        "\n",
        "with open('norm.pkl', 'rb') as f:\n",
        "    norm = pkl.load(f)\n",
        "    norm_mean = norm['mean']\n",
        "    norm_std = norm['std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jNHEdVYdIt-b"
      },
      "outputs": [],
      "source": [
        "# we use Adam optimizer, use cross entropy loss as our loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6foqWmnIt-b"
      },
      "source": [
        "Train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BX7qAZYtIt-b"
      },
      "outputs": [],
      "source": [
        "# this function is used during training process, to calculation the loss and accuracy\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Kdb1PGKlIt-b"
      },
      "outputs": [],
      "source": [
        "total_loss_train, total_acc_train = [],[]\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    train_acc = AverageMeter()\n",
        "    curr_iter = (epoch - 1) * len(train_loader)\n",
        "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        images, labels, additional_info = data\n",
        "        N = images.size(0)\n",
        "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "        if model_name in ['densenetcustom']:\n",
        "            additional_info_tensor = torch.cat([v.unsqueeze(1) for v in additional_info.values()], dim=1)\n",
        "            additional_info_tensor = Variable(additional_info_tensor).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, additional_info)\n",
        "        else:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "        train_loss.update(loss.item())\n",
        "        curr_iter += 1\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
        "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
        "            total_loss_train.append(train_loss.avg)\n",
        "            total_acc_train.append(train_acc.avg)\n",
        "    return train_loss.avg, train_acc.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AbZy1J51It-b"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, criterion, optimizer, epoch):\n",
        "    model.eval()\n",
        "    val_loss = AverageMeter()\n",
        "    val_acc = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
        "            images, labels, additional_info = data\n",
        "            N = images.size(0)\n",
        "            images = Variable(images).to(device)\n",
        "            labels = Variable(labels).to(device)\n",
        "\n",
        "            if model_name in ['densenetcustom']:\n",
        "                additional_info_tensor = torch.cat([v.unsqueeze(1) for v in additional_info.values()], dim=1)\n",
        "                additional_info_tensor = Variable(additional_info_tensor).to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images, additional_info)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "\n",
        "            prediction = outputs.max(1, keepdim=True)[1]\n",
        "\n",
        "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "\n",
        "            val_loss.update(criterion(outputs, labels).item())\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
        "    print('------------------------------------------------------------')\n",
        "    return val_loss.avg, val_acc.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UFIsO9iIt-c",
        "outputId": "60f9600e-d43b-47e3-fef3-8a03255cbee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on epoch 1\n"
          ]
        }
      ],
      "source": [
        "epoch_num = 10\n",
        "best_val_acc = 0\n",
        "total_loss_val, total_acc_val = [],[]\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    print('Training on epoch {}'.format(epoch))\n",
        "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n",
        "    total_loss_val.append(loss_val)\n",
        "    total_acc_val.append(acc_val)\n",
        "    if acc_val > best_val_acc:\n",
        "        best_val_acc = acc_val\n",
        "        print('*****************************************************')\n",
        "        print('Best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
        "        print('*****************************************************')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
