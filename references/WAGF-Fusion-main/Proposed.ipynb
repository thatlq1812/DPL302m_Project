{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6128847,"sourceType":"datasetVersion","datasetId":3513799}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport numpy as np\nfrom scipy.fftpack import dct\nimport numpy as np\nimport tensorflow.keras as K\nimport tensorflow.keras.backend as Kback\n!pip install tensorflow-wavelets\nimport tensorflow_wavelets.Layers.DWT as DWT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"train_datagen = K.preprocessing.image.ImageDataGenerator(rescale = 1./255)   \n\ntrain_dataset  = train_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/train_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   shuffle=True,\n                                                   batch_size = 64)\nvalidation_dataset  = train_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/train_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   shuffle=True,\n                                                   batch_size = 64)\n\n\ntest_datagen = K.preprocessing.image.ImageDataGenerator(rescale = 1./255)   \n\ntest_dataset  = test_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/test_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   shuffle=False,\n                                                   batch_size = 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset.class_indices)\nprint(validation_dataset.class_indices)\nprint(test_dataset.class_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    true_positives = Kback.sum(Kback.round(Kback.clip(y_true * y_pred, 0, 1)))\n    possible_positives = Kback.sum(Kback.round(Kback.clip(y_true, 0, 1)))\n    predicted_positives = Kback.sum(Kback.round(Kback.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + Kback.epsilon())\n    recall = true_positives / (possible_positives + Kback.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+Kback.epsilon())\n    return f1_val\n\nMETRICS = [\n      \"accuracy\",\n      K.metrics.Precision(name='precision'),\n      K.metrics.Recall(name='recall'),\n      K.metrics.AUC(name='auc'),\n      f1_score\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"#### FFT","metadata":{}},{"cell_type":"code","source":"def fft_2d(feature_map):\n    feature_map = tf.cast(feature_map, tf.complex64)\n    X1 = tf.signal.fft2d(feature_map)\n    X1 = tf.abs(X1)\n    return X1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SaFA","metadata":{}},{"cell_type":"code","source":"def Similarity(x):\n    batch,H,W,C = x.shape\n    x = K.layers.Reshape((H, W))(x)\n    x_bar = K.layers.Permute((2, 1))(x)\n    sim = K.layers.Dot(axes=(1,2), normalize=True)([x,x_bar])\n    sim = tf.expand_dims(sim, axis=-1)\n    return sim\n\ndef FeatureChange(x):\n    batch,H,W,C = x.shape\n    x_h = K.layers.Reshape((H, W))(x)\n    lstm_h,_,_ = K.layers.LSTM(H, return_sequences=True, return_state=True)(x_h)\n    x_w = K.layers.Permute((2, 1))(x_h)\n    lstm_w,_,_ = K.layers.LSTM(W, return_sequences=True, return_state=True)(x_w)\n    print(lstm_w.shape)\n    lstm = lstm_h+lstm_w\n    lstm = tf.expand_dims(lstm, axis=-1)\n    return lstm\n\ndef SaFA(inputs):\n    x = K.layers.SeparableConv2D(256, 5, padding=\"same\")(inputs)\n    x = K.layers.SeparableConv2D(64, 3, padding=\"same\")(x)\n    x = K.layers.SeparableConv2D(1, 1, padding=\"same\")(x)\n    lstm = FeatureChange(x)\n    sim = Similarity(x)\n    attn = sim*inputs\n    x = K.layers.SeparableConv2D(256, 1, strides=(2, 2), padding=\"same\")(attn)\n    x = K.layers.SeparableConv2D(64, 3, padding=\"same\")(x)\n    x = K.layers.SeparableConv2D(1, 1, padding=\"same\", activation='sigmoid')(x)\n    inputs = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(inputs)\n    x = inputs*x\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gradients","metadata":{}},{"cell_type":"code","source":"class Gradients(K.layers.Layer):\n    def call(self, inputs):\n        alpha = inputs\n        gradients_alpha = tf.gradients(alpha, [alpha])[0]\n        a = tf.reduce_mean(gradients_alpha, axis=[-1,-2,-3], keepdims=True)\n        return a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Soft Attention","metadata":{}},{"cell_type":"code","source":"class SoftAttention(K.layers.Layer):\n    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n        self.channels=int(ch)\n        self.multiheads = m\n        self.aggregate_channels = aggregate\n        self.concat_input_with_scaled = concat_with_x\n\n        \n        super(SoftAttention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n\n        self.i_shape = input_shape\n\n        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n    \n        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n        \n        if self.aggregate_channels==False:\n\n            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n        else:\n            if self.concat_input_with_scaled:\n                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n            else:\n                self.out_features_shape = input_shape\n        \n\n        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n                                        initializer='he_uniform',\n                                        name='kernel_conv3d')\n        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n                                      initializer='zeros',\n                                      name='bias_conv3d')\n\n        super(SoftAttention, self).build(input_shape)\n\n    def call(self, x):\n\n        exp_x = Kback.expand_dims(x,axis=-1)\n\n        c3d = Kback.conv3d(exp_x,\n                     kernel=self.kernel_conv3d,\n                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n        conv3d = Kback.bias_add(c3d,\n                        self.bias_conv3d)\n        conv3d = K.layers.Activation('relu')(conv3d)\n\n        conv3d = Kback.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n\n        \n        conv3d = Kback.squeeze(conv3d, axis=-1)\n        conv3d = Kback.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n\n        softmax_alpha = Kback.softmax(conv3d, axis=-1) \n        softmax_alpha = K.layers.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n\n        \n        if self.aggregate_channels==False:\n            exp_softmax_alpha = Kback.expand_dims(softmax_alpha, axis=-1)       \n            exp_softmax_alpha = Kback.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n   \n            x_exp = Kback.expand_dims(x,axis=-2)\n   \n            u = K.layers.Multiply()([exp_softmax_alpha, x_exp])   \n  \n            u = K.layers.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n\n        else:\n            exp_softmax_alpha = Kback.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n\n            exp_softmax_alpha = Kback.sum(exp_softmax_alpha,axis=-1)\n\n            exp_softmax_alpha = Kback.expand_dims(exp_softmax_alpha, axis=-1)\n\n            u = K.layers.Multiply()([exp_softmax_alpha, x])   \n\n        if self.concat_input_with_scaled:\n            o = K.layers.Concatenate(axis=-1)([u,x])\n        else:\n            o = u\n        \n        return [o, softmax_alpha]\n\n    def compute_output_shape(self, input_shape): \n        return [self.out_features_shape, self.out_attention_maps_shape]\n\n    \n    def get_config(self):\n        return super(SoftAttention,self).get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Deep Learner","metadata":{}},{"cell_type":"code","source":"input_layer = K.Input(shape=(256,256,3))\ndeep_learner = K.applications.DenseNet121(include_top = False, weights = \"imagenet\", input_tensor = input_layer)\nfor layer in deep_learner.layers:\n    layer.trainable = True\n# for i, layer in enumerate(deep_learner.layers):\n#     print(i, layer.name, \"-\", layer.trainable)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model","metadata":{}},{"cell_type":"code","source":"input_img = K.layers.Input(shape=(256,256,3)) \nfeat_img = deep_learner(input_img)\nwav = DWT.DWT(name=\"haar\",concat=0)(feat_img)\nwav = K.layers.SeparableConv2D(1024, 1, padding=\"same\")(wav)\nattention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(feat_img.shape[-1]),name='soft_attention')(feat_img)\nattention_layer = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer)\n#conv = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(feat_img)\n\ngrad_attn = Gradients()(attention_layer)\ngrad_wav = Gradients()(wav)\ngrad_attn = 1-(grad_attn/(grad_attn+grad_wav))\ngrad_wav = 1-(grad_wav/(grad_attn+grad_wav))\nattention_layer = grad_attn*attention_layer+grad_wav*wav\nconv = K.layers.concatenate([SaFA(feat_img),attention_layer])\nconv  = K.layers.Activation('relu')(conv)\nflat = K.layers.GlobalAveragePooling2D()(conv)\noutput = K.layers.Dense(7, activation='softmax')(flat)\n\nmodel = K.Model(inputs=input_img, outputs=output)\noptimizer = K.optimizers.Adam(lr=0.01)\nmodel.compile(loss=[\"categorical_crossentropy\"], metrics=METRICS, optimizer = optimizer)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"model_checkpoint_callback = K.callbacks.ModelCheckpoint(\n    filepath='densenet121_SA.hdf5',\n    monitor='val_f1_score',\n    save_best_only=True,\n    save_weights_only=True,\n    mode='max',\n    verbose=1\n    )\n\nhistory = model.fit(train_dataset,\n                    epochs = 25,\n                    validation_data = validation_dataset,\n                    verbose = 1,\n                    callbacks=[model_checkpoint_callback],\n                    shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training plots","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef Train_Val_Plot(acc, val_acc, loss, val_loss, auc, val_auc, precision, val_precision, recall, val_recall, f1_score, val_f1_score):\n    fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n    fig.suptitle(\"MODEL'S METRICS VISUALIZATION\")\n\n    axes[0, 0].plot(range(1, len(acc) + 1), acc)\n    axes[0, 0].plot(range(1, len(val_acc) + 1), val_acc)\n    axes[0, 0].set_title('History of Accuracy')\n    axes[0, 0].set_xlabel('Epochs')\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].legend(['training', 'validation'])\n\n    axes[0, 1].plot(range(1, len(loss) + 1), loss)\n    axes[0, 1].plot(range(1, len(val_loss) + 1), val_loss)\n    axes[0, 1].set_title('History of Loss')\n    axes[0, 1].set_xlabel('Epochs')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend(['training', 'validation'])\n\n    axes[0, 2].plot(range(1, len(auc) + 1), auc)\n    axes[0, 2].plot(range(1, len(val_auc) + 1), val_auc)\n    axes[0, 2].set_title('History of AUC')\n    axes[0, 2].set_xlabel('Epochs')\n    axes[0, 2].set_ylabel('AUC')\n    axes[0, 2].legend(['training', 'validation'])\n\n    axes[1, 0].plot(range(1, len(precision) + 1), precision)\n    axes[1, 0].plot(range(1, len(val_precision) + 1), val_precision)\n    axes[1, 0].set_title('History of Precision')\n    axes[1, 0].set_xlabel('Epochs')\n    axes[1, 0].set_ylabel('Precision')\n    axes[1, 0].legend(['training', 'validation'])\n\n    axes[1, 1].plot(range(1, len(recall) + 1), recall)\n    axes[1, 1].plot(range(1, len(val_recall) + 1), val_recall)\n    axes[1, 1].set_title('History of Recall')\n    axes[1, 1].set_xlabel('Epochs')\n    axes[1, 1].set_ylabel('Recall')\n    axes[1, 1].legend(['training', 'validation'])\n\n    axes[1, 2].plot(range(1, len(f1_score) + 1), f1_score)\n    axes[1, 2].plot(range(1, len(val_f1_score) + 1), val_f1_score)\n    axes[1, 2].set_title('History of F1 score')\n    axes[1, 2].set_xlabel('Epochs')\n    axes[1, 2].set_ylabel('Recall')  # Corrected from 'Recall' to 'F1 score'\n    axes[1, 2].legend(['training', 'validation'])\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function with your history data\nTrain_Val_Plot(history.history['accuracy'], history.history['val_accuracy'],\n               history.history['loss'], history.history['val_loss'],\n               history.history['auc'], history.history['val_auc'],\n               history.history['precision'], history.history['val_precision'],\n               history.history['recall'], history.history['val_recall'],\n               history.history['f1_score'], history.history['val_f1_score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"/kaggle/working/densenet121_SA.hdf5\")\n\n# Evaluate the model\nloss, accuracy, precision, recall, auc, f1_score = model.evaluate(test_dataset)\nprint(\"Accuracy\", accuracy)\nprint(\"Loss\", loss)\nprint(\"Precision\", precision)\nprint(\"Recall\", recall)\nprint(\"AUC\", auc)\nprint(\"F1-score\", f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nY_pred = model.predict_generator(test_dataset, 1157)\ny_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\ndisp = ConfusionMatrixDisplay(confusion_matrix(test_dataset.classes, y_pred),display_labels=['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'])\ndisp.plot()\nplt.show()\nprint('Classification Report')\ntarget_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\nprint(classification_report(test_dataset.classes, y_pred, target_names=target_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}