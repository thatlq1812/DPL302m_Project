{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6128847,"datasetId":3513799,"databundleVersionId":6207590},{"sourceType":"datasetVersion","sourceId":7624038,"datasetId":3526941,"databundleVersionId":7719931}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport numpy as np\nfrom scipy.fftpack import dct\nimport numpy as np\nimport tensorflow.keras as K\nimport tensorflow.keras.backend as Kback\n!pip install tensorflow-wavelets\nimport tensorflow_wavelets.Layers.DWT as DWT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"train_datagen = K.preprocessing.image.ImageDataGenerator(rescale = 1./255, validation_split = 0.3)   \n\ntrain_dataset  = train_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/train_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   shuffle=True,\n                                                   batch_size = 64)\nvalidation_dataset  = train_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/train_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'validation',\n                                                   shuffle=True,\n                                                   batch_size = 64)\n\n\ntest_datagen = K.preprocessing.image.ImageDataGenerator(rescale = 1./255)   \n\ntest_dataset  = test_datagen.flow_from_directory(directory = '/kaggle/input/ham10000-data/HAM10000_DATA/test_dir',\n                                                   target_size = (256,256),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   shuffle=False,\n                                                   batch_size = 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset.class_indices)\nprint(validation_dataset.class_indices)\nprint(test_dataset.class_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def f1_score(y_true, y_pred):\n    true_positives = Kback.sum(Kback.round(Kback.clip(y_true * y_pred, 0, 1)))\n    possible_positives = Kback.sum(Kback.round(Kback.clip(y_true, 0, 1)))\n    predicted_positives = Kback.sum(Kback.round(Kback.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + Kback.epsilon())\n    recall = true_positives / (possible_positives + Kback.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+Kback.epsilon())\n    return f1_val\n\nMETRICS = [\n      \"accuracy\",\n      K.metrics.Precision(name='precision'),\n      K.metrics.Recall(name='recall'),\n      K.metrics.AUC(name='auc'),\n      f1_score\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"#### FFT","metadata":{}},{"cell_type":"code","source":"def fft_2d(feature_map):\n    feature_map = tf.cast(feature_map, tf.complex64)\n    X1 = tf.signal.fft2d(feature_map)\n    X1 = tf.abs(X1)\n    return X1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SaFA","metadata":{}},{"cell_type":"code","source":"def Similarity(x):\n    batch,H,W,C = x.shape\n    x = K.layers.Reshape((H, W))(x)\n    x_bar = K.layers.Permute((2, 1))(x)\n    sim = K.layers.Dot(axes=(1,2), normalize=True)([x,x_bar])\n    sim = tf.expand_dims(sim, axis=-1)\n    return sim\n\ndef FeatureChange(x):\n    batch,H,W,C = x.shape\n    x_h = K.layers.Reshape((H, W))(x)\n    lstm_h,_,_ = K.layers.LSTM(H, return_sequences=True, return_state=True)(x_h)\n    x_w = K.layers.Permute((2, 1))(x_h)\n    lstm_w,_,_ = K.layers.LSTM(W, return_sequences=True, return_state=True)(x_w)\n    lstm = lstm_h+lstm_w\n    lstm = tf.expand_dims(lstm, axis=-1)\n    return lstm\n\ndef SaFA(inputs):\n    x = K.layers.SeparableConv2D(256, 5, padding=\"same\")(inputs)\n    x = K.layers.SeparableConv2D(64, 3, padding=\"same\")(x)\n    x = K.layers.SeparableConv2D(1, 1, padding=\"same\")(x)\n    lstm = FeatureChange(x)\n    sim = Similarity(x)\n    attn = sim*inputs\n    x = K.layers.SeparableConv2D(256, 1, strides=(2, 2), padding=\"same\")(attn)\n    x = K.layers.SeparableConv2D(64, 3, padding=\"same\")(x)\n    x = K.layers.SeparableConv2D(1, 1, padding=\"same\", activation='sigmoid')(x)\n    inputs = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(inputs)\n    x = inputs*x\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gradients","metadata":{}},{"cell_type":"code","source":"class Gradients(K.layers.Layer):\n    def call(self, inputs):\n        alpha = inputs\n        gradients_alpha = tf.gradients(alpha, [alpha])[0]\n        a = tf.reduce_mean(gradients_alpha, axis=[-1,-2,-3], keepdims=True)\n        return a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Soft Attention","metadata":{}},{"cell_type":"code","source":"class SoftAttention(K.layers.Layer):\n    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n        self.channels=int(ch)\n        self.multiheads = m\n        self.aggregate_channels = aggregate\n        self.concat_input_with_scaled = concat_with_x\n\n        \n        super(SoftAttention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n\n        self.i_shape = input_shape\n\n        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n    \n        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n        \n        if self.aggregate_channels==False:\n\n            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n        else:\n            if self.concat_input_with_scaled:\n                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n            else:\n                self.out_features_shape = input_shape\n        \n\n        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n                                        initializer='he_uniform',\n                                        name='kernel_conv3d')\n        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n                                      initializer='zeros',\n                                      name='bias_conv3d')\n\n        super(SoftAttention, self).build(input_shape)\n\n    def call(self, x):\n\n        exp_x = Kback.expand_dims(x,axis=-1)\n\n        c3d = Kback.conv3d(exp_x,\n                     kernel=self.kernel_conv3d,\n                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n        conv3d = Kback.bias_add(c3d,\n                        self.bias_conv3d)\n        conv3d = K.layers.Activation('relu')(conv3d)\n\n        conv3d = Kback.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n\n        \n        conv3d = Kback.squeeze(conv3d, axis=-1)\n        conv3d = Kback.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n\n        softmax_alpha = Kback.softmax(conv3d, axis=-1) \n        softmax_alpha = K.layers.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n\n        \n        if self.aggregate_channels==False:\n            exp_softmax_alpha = Kback.expand_dims(softmax_alpha, axis=-1)       \n            exp_softmax_alpha = Kback.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n   \n            x_exp = Kback.expand_dims(x,axis=-2)\n   \n            u = K.layers.Multiply()([exp_softmax_alpha, x_exp])   \n  \n            u = K.layers.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n\n        else:\n            exp_softmax_alpha = Kback.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n\n            exp_softmax_alpha = Kback.sum(exp_softmax_alpha,axis=-1)\n\n            exp_softmax_alpha = Kback.expand_dims(exp_softmax_alpha, axis=-1)\n\n            u = K.layers.Multiply()([exp_softmax_alpha, x])   \n\n        if self.concat_input_with_scaled:\n            o = K.layers.Concatenate(axis=-1)([u,x])\n        else:\n            o = u\n        \n        return [o, softmax_alpha]\n\n    def compute_output_shape(self, input_shape): \n        return [self.out_features_shape, self.out_attention_maps_shape]\n\n    \n    def get_config(self):\n        return super(SoftAttention,self).get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Deep Learner","metadata":{}},{"cell_type":"code","source":"input_layer = K.Input(shape=(256,256,3))\ndeep_learner = K.applications.DenseNet121(include_top = False, weights = \"imagenet\", input_tensor = input_layer)\nfor layer in deep_learner.layers:\n    layer.trainable = True\n# for i, layer in enumerate(deep_learner.layers):\n#     print(i, layer.name, \"-\", layer.trainable)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model","metadata":{}},{"cell_type":"code","source":"input_img = K.layers.Input(shape=(256,256,3)) \nfeat_img = deep_learner(input_img)\nwav = DWT.DWT(name=\"haar\",concat=0)(feat_img)\nwav = K.layers.SeparableConv2D(1024, 1, padding=\"same\")(wav)\nattention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(feat_img.shape[-1]),name='soft_attention')(feat_img)\nattention_layer = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer)\n#conv = K.layers.MaxPooling2D(pool_size=(2, 2),padding=\"same\")(feat_img)\n\ngrad_attn = Gradients()(attention_layer)\ngrad_wav = Gradients()(wav)\ngrad_attn = 1-(grad_attn/(grad_attn+grad_wav))\ngrad_wav = 1-(grad_wav/(grad_attn+grad_wav))\nattention_layer = grad_attn*attention_layer+grad_wav*wav\nconv = K.layers.concatenate([SaFA(feat_img),attention_layer])\nconv  = K.layers.Activation('relu')(conv)\nflat = K.layers.GlobalAveragePooling2D()(conv)\noutput = K.layers.Dense(7, activation='softmax')(flat)\n\nmodel = K.Model(inputs=input_img, outputs=output)\noptimizer = K.optimizers.Adam(lr=0.01)\nmodel.compile(loss=[\"categorical_crossentropy\"], metrics=METRICS, optimizer = optimizer)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/segmentation-models/densenet121_SA.hdf5\")\n\n# Evaluate the model\nloss, accuracy, precision, recall, auc, f1_score = model.evaluate(test_dataset)\nprint(\"Accuracy\", accuracy)\nprint(\"Loss\", loss)\nprint(\"Precision\", precision)\nprint(\"Recall\", recall)\nprint(\"AUC\", auc)\nprint(\"F1-score\", f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nY_pred = model.predict_generator(test_dataset, 1157)\ny_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\ndisp = ConfusionMatrixDisplay(confusion_matrix(test_dataset.classes, y_pred),display_labels=['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc'])\ndisp.plot()\nplt.show()\nprint('Classification Report')\ntarget_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\nprint(classification_report(test_dataset.classes, y_pred, target_names=target_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modeller = K.Model(inputs=model.input, outputs=model.get_layer(name=\"global_average_pooling2d\").output)\n# Define the number of classes\nnum_classes = 7\n\n# Initialize empty arrays for features and labels\nall_features = []\nall_labels = []\n\nmax_iterations = 1103\ni=0\n\n# Extract features and labels from the Keras test generator\nfor batch_features, batch_labels in test_dataset:\n    features = modeller.predict(batch_features)\n    all_features.append(features)\n    all_labels.append(batch_labels)\n    i+=1\n    if i >= max_iterations - 1:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nj_range = range(1102)  # Adjust the range as needed\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define colors for different classes\ncolors = ['b', 'r', 'y', 'g', 'c', 'm', 'k']\n\n# Set the size and alpha for the points\npoint_size = 40\npoint_alpha = 0.3\n\nfor j in j_range:\n    # Check if the number of samples or features is less than 3\n#     if all_features[j].shape[0] < 3 or all_features[j].shape[1] < 3:\n#         continue\n    \n    # Apply PCA to reduce the dimension to 3\n    pca = PCA(n_components=3)\n    \n    features_pca_0 = pca.fit_transform(all_features[j])\n\n    # Get the labels for this 'j'\n    labels = all_labels[j]\n\n    # Plot each class with circular markers and different colors\n    for i in range(num_classes):\n        class_indices = np.where(labels[:, i] == 1)[0]\n        current_color = colors[i % len(colors)]  # Get the color for this class\n        ax.scatter(features_pca_0[class_indices, 0], features_pca_0[class_indices, 1], features_pca_0[class_indices, 2], c=current_color, marker='o', s=point_size, alpha=point_alpha, ec='black')\n\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_zlabel('Z-axis')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\nimport tensorflow as tf\n\ndef visualize_class_activation_map(model, img_path, output_path, layer_name, thr=None, verbose=True):\n    original_img = Image.open(img_path).convert('RGB')\n    original_img = original_img.resize((256, 256))\n    img_arr = np.array(original_img) / 255.\n    width, height, _ = img_arr.shape\n    cam_model = tf.keras.Model(model.input, model.get_layer(layer_name).output)\n    cam = cam_model(np.array([img_arr])).numpy()[0]\n    print(img_arr.shape)\n    mask = np.mean(cam, axis=-1)\n    mask = (mask - np.min(mask)) / (np.max(mask) - np.min(mask))\n    print(mask.shape)\n    mask_width, mask_height = mask.shape\n    scaled_mask = tf.keras.layers.UpSampling2D(size=(width // mask_width, height // mask_height), interpolation=\"bilinear\")(\n        np.reshape(mask, (1, mask_width, mask_height, 1)))[0].numpy()\n    print(scaled_mask.shape)\n    new_shape = (256, 256, 3)\n    img_arr = img_arr[:new_shape[0], :new_shape[1]]\n    attentive_img = (img_arr * scaled_mask) * 255\n\n    # Create a heatmap using 'jet' colormap\n    heatmap = cm.jet(scaled_mask.squeeze())  # Squeeze the extra dimension\n    return original_img, heatmap, attentive_img\n\nimg_path = '/kaggle/input/ham10000-data/HAM10000_DATA/test_dir/vasc/ISIC_0027903.jpg'\noutput_path = '007.png'\nlayer_name = 'separable_conv2d_6'\noriginal_img, heatmap, attentive_img = visualize_class_activation_map(model, img_path, output_path, layer_name)\n\n# Plot the original image and the heatmap\nplt.figure(figsize=(6, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(original_img)\nplt.title('Original Image')\n\nplt.subplot(1, 2, 2)\nplt.imshow(heatmap, alpha=0.65)\nplt.title('Heatmap')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}