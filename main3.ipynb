{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Google Colab library\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    print(\"In Google Colab\")\n",
    "except:\n",
    "    in_colab = False\n",
    "    print(\"Not in Google Colab\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python library\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "\n",
    "import yaml\n",
    "import albumentations as A\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Sklearn library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Pytorch library\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# CV2 library\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Cuda version:  12.4\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print(\"Cuda version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name:  Cancer Detection\n",
      "Project version:  DPL302m Project - Fall 2024 of Group 1\n",
      "Project author:  Group 1\n",
      "Project version:  1.0.0\n",
      "Data path:  {'data_path': 'data/', 'raw_path': 'data/raw/', 'raw_csv_path': 'data/raw/HAM10000_metadata.csv', 'raw_images_path': 'data/raw/images/', 'processed_path': 'data/processed/', 'processed_images_path': 'data/processed/images/'}\n",
      "N workers:  16\n",
      "Batch size:  32\n",
      "Preprocessing configuration\n",
      "Augmentation ratios:  1\n",
      "Resize shape:  (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Project information\n",
    "print('Project name: ', config['project']['name'])\n",
    "print('Project version: ', config['project']['description'])\n",
    "print('Project author: ', config['project']['author'])\n",
    "print('Project version: ', config['project']['version'])\n",
    "\n",
    "# Data location\n",
    "print('Data path: ', config['data'])\n",
    "data_path = config['data']['raw_path']\n",
    "csv_path = config['data']['raw_csv_path']\n",
    "images_path = config['data']['raw_images_path']\n",
    "\n",
    "# Training configuration\n",
    "if in_colab:\n",
    "    n_workers = multiprocessing.cpu_count()\n",
    "    batch_size = 128\n",
    "else:\n",
    "    n_workers = config['hyperparameters']['n_workers']\n",
    "    batch_size = config['hyperparameters']['batch_size']\n",
    "print('N workers: ', n_workers)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "# Preprocessing configuration\n",
    "print('Preprocessing configuration')\n",
    "augmentation_ratio = config['preprocessing']['augmentation']['ratio']\n",
    "resize_shape = tuple(config['preprocessing']['resize'])\n",
    "print('Augmentation ratios: ', augmentation_ratio)\n",
    "print('Resize shape: ', resize_shape)\n",
    "\n",
    "# Create a dictionary for images location\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already processed\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "if os.path.exists(f'data/processed/HAM10000_{resize_shape[0]}x{resize_shape[1]}_train.pt'):\n",
    "    print('Data is already processed')\n",
    "else:\n",
    "    print('Data is not processed yet')\n",
    "    from scripts.preprocessing import *\n",
    "    preprocessor = DataProcessor(config_path='config.yaml')\n",
    "    preprocessor.run(aug=True, rate = 1)\n",
    "    print('Data is processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Model building\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    # Variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"Densenet121\":\n",
    "        weights = 'DEFAULT' if use_pretrained else None\n",
    "        model_ft = models.densenet121(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = config['preprocessing']['resize'][0]\n",
    "    \n",
    "    elif model_name == \"Resnet18\":\n",
    "        # Add initialization for Resnet18 if needed\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Step 3: Model training\n",
    "model_name = config['hyperparameters']['model_name']\n",
    "num_classes = config['hyperparameters']['num_classes']\n",
    "feature_extract = config['hyperparameters']['feature_extract']\n",
    "use_pretrained = config['hyperparameters']['use_pretrained']\n",
    "\n",
    "\n",
    "# Initialize the model for this run and move it to the device\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained)\n",
    "model = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fxlqt\\AppData\\Local\\Temp\\ipykernel_5568\\798468676.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder = torch.load(encoder_file)\n",
      "C:\\Users\\fxlqt\\AppData\\Local\\Temp\\ipykernel_5568\\798468676.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_file)\n"
     ]
    }
   ],
   "source": [
    "class HAM10000(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        # Load the merged data\n",
    "        data = torch.load(data_file)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['data']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.images[index]\n",
    "        y = torch.tensor(self.labels[index])\n",
    "        return X, y\n",
    "\n",
    "# Define location of prepared data\n",
    "size = config['preprocessing']['resize'][0]\n",
    "\n",
    "encoder_file = f'encoder/{size}x{size}.pt'\n",
    "train_data_file = f'data/processed/HAM10000_{size}x{size}_train.pt'\n",
    "test_data_file = f'data/processed/HAM10000_{size}x{size}_test.pt'\n",
    "\n",
    "# Load the encoder\n",
    "encoder = torch.load(encoder_file)\n",
    "\n",
    "# Load the training data\n",
    "train_data = HAM10000(train_data_file)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load the test data\n",
    "test_data = HAM10000(test_data_file)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Adam optimizer, use cross entropy loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % (len(train_loader) // 10) == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...  on image size:  [224, 224]  with epoch:  50\n",
      "Training on epoch 1\n",
      "[epoch 1], [iter 112 / 1121], [train loss 1.93401], [train acc 0.21680]\n",
      "[epoch 1], [iter 224 / 1121], [train loss 1.90373], [train acc 0.23061]\n",
      "[epoch 1], [iter 336 / 1121], [train loss 1.88622], [train acc 0.24284]\n",
      "[epoch 1], [iter 448 / 1121], [train loss 1.87096], [train acc 0.25119]\n",
      "[epoch 1], [iter 560 / 1121], [train loss 1.85811], [train acc 0.26032]\n",
      "[epoch 1], [iter 672 / 1121], [train loss 1.84689], [train acc 0.26828]\n",
      "[epoch 1], [iter 784 / 1121], [train loss 1.83254], [train acc 0.27691]\n",
      "[epoch 1], [iter 896 / 1121], [train loss 1.82051], [train acc 0.28418]\n",
      "[epoch 1], [iter 1008 / 1121], [train loss 1.80960], [train acc 0.29164]\n",
      "[epoch 1], [iter 1120 / 1121], [train loss 1.80229], [train acc 0.29704]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 1.81685], [val acc 0.37198]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "Best record: [epoch 1], [val loss 1.81685], [val acc 0.37198]\n",
      "*****************************************************\n",
      "Training on epoch 2\n",
      "[epoch 2], [iter 112 / 1121], [train loss 1.70532], [train acc 0.34598]\n",
      "[epoch 2], [iter 224 / 1121], [train loss 1.67456], [train acc 0.36761]\n",
      "[epoch 2], [iter 336 / 1121], [train loss 1.65581], [train acc 0.37147]\n",
      "[epoch 2], [iter 448 / 1121], [train loss 1.64298], [train acc 0.37730]\n",
      "[epoch 2], [iter 560 / 1121], [train loss 1.63590], [train acc 0.37913]\n",
      "[epoch 2], [iter 672 / 1121], [train loss 1.62253], [train acc 0.38495]\n",
      "[epoch 2], [iter 784 / 1121], [train loss 1.61211], [train acc 0.39059]\n",
      "[epoch 2], [iter 896 / 1121], [train loss 1.60250], [train acc 0.39495]\n",
      "[epoch 2], [iter 1008 / 1121], [train loss 1.58805], [train acc 0.40058]\n",
      "[epoch 2], [iter 1120 / 1121], [train loss 1.57848], [train acc 0.40466]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 2.16024], [val acc 0.16986]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 3\n",
      "[epoch 3], [iter 112 / 1121], [train loss 1.44030], [train acc 0.46150]\n",
      "[epoch 3], [iter 224 / 1121], [train loss 1.41775], [train acc 0.46833]\n",
      "[epoch 3], [iter 336 / 1121], [train loss 1.39333], [train acc 0.47387]\n",
      "[epoch 3], [iter 448 / 1121], [train loss 1.39639], [train acc 0.47077]\n",
      "[epoch 3], [iter 560 / 1121], [train loss 1.38132], [train acc 0.47684]\n",
      "[epoch 3], [iter 672 / 1121], [train loss 1.37336], [train acc 0.47786]\n",
      "[epoch 3], [iter 784 / 1121], [train loss 1.35576], [train acc 0.48645]\n",
      "[epoch 3], [iter 896 / 1121], [train loss 1.34249], [train acc 0.49229]\n",
      "[epoch 3], [iter 1008 / 1121], [train loss 1.33489], [train acc 0.49361]\n",
      "[epoch 3], [iter 1120 / 1121], [train loss 1.32640], [train acc 0.49816]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 2.42375], [val acc 0.09829]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 4\n",
      "[epoch 4], [iter 112 / 1121], [train loss 1.22482], [train acc 0.53711]\n",
      "[epoch 4], [iter 224 / 1121], [train loss 1.19197], [train acc 0.55092]\n",
      "[epoch 4], [iter 336 / 1121], [train loss 1.18292], [train acc 0.55208]\n",
      "[epoch 4], [iter 448 / 1121], [train loss 1.16960], [train acc 0.55699]\n",
      "[epoch 4], [iter 560 / 1121], [train loss 1.16680], [train acc 0.55658]\n",
      "[epoch 4], [iter 672 / 1121], [train loss 1.16194], [train acc 0.55766]\n",
      "[epoch 4], [iter 784 / 1121], [train loss 1.15260], [train acc 0.56067]\n",
      "[epoch 4], [iter 896 / 1121], [train loss 1.14273], [train acc 0.56494]\n",
      "[epoch 4], [iter 1008 / 1121], [train loss 1.13692], [train acc 0.56768]\n",
      "[epoch 4], [iter 1120 / 1121], [train loss 1.13018], [train acc 0.57065]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 1.59503], [val acc 0.51058]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "Best record: [epoch 4], [val loss 1.59503], [val acc 0.51058]\n",
      "*****************************************************\n",
      "Training on epoch 5\n",
      "[epoch 5], [iter 112 / 1121], [train loss 0.99274], [train acc 0.62109]\n",
      "[epoch 5], [iter 224 / 1121], [train loss 0.99421], [train acc 0.62165]\n",
      "[epoch 5], [iter 336 / 1121], [train loss 0.98978], [train acc 0.62398]\n",
      "[epoch 5], [iter 448 / 1121], [train loss 0.97478], [train acc 0.62786]\n",
      "[epoch 5], [iter 560 / 1121], [train loss 0.97189], [train acc 0.62857]\n",
      "[epoch 5], [iter 672 / 1121], [train loss 0.96399], [train acc 0.63100]\n",
      "[epoch 5], [iter 784 / 1121], [train loss 0.95933], [train acc 0.63381]\n",
      "[epoch 5], [iter 896 / 1121], [train loss 0.95648], [train acc 0.63414]\n",
      "[epoch 5], [iter 1008 / 1121], [train loss 0.95480], [train acc 0.63430]\n",
      "[epoch 5], [iter 1120 / 1121], [train loss 0.94704], [train acc 0.63717]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 1.84771], [val acc 0.38105]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 6\n",
      "[epoch 6], [iter 112 / 1121], [train loss 0.81218], [train acc 0.68080]\n",
      "[epoch 6], [iter 224 / 1121], [train loss 0.80897], [train acc 0.68638]\n",
      "[epoch 6], [iter 336 / 1121], [train loss 0.81736], [train acc 0.68545]\n",
      "[epoch 6], [iter 448 / 1121], [train loss 0.81298], [train acc 0.68513]\n",
      "[epoch 6], [iter 560 / 1121], [train loss 0.80828], [train acc 0.68717]\n",
      "[epoch 6], [iter 672 / 1121], [train loss 0.80825], [train acc 0.68745]\n",
      "[epoch 6], [iter 784 / 1121], [train loss 0.81494], [train acc 0.68567]\n",
      "[epoch 6], [iter 896 / 1121], [train loss 0.81214], [train acc 0.68593]\n",
      "[epoch 6], [iter 1008 / 1121], [train loss 0.80802], [train acc 0.68769]\n",
      "[epoch 6], [iter 1120 / 1121], [train loss 0.80279], [train acc 0.68926]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 2.22760], [val acc 0.28226]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 7\n",
      "[epoch 7], [iter 112 / 1121], [train loss 0.72085], [train acc 0.71708]\n",
      "[epoch 7], [iter 224 / 1121], [train loss 0.70582], [train acc 0.72405]\n",
      "[epoch 7], [iter 336 / 1121], [train loss 0.69155], [train acc 0.72954]\n",
      "[epoch 7], [iter 448 / 1121], [train loss 0.68832], [train acc 0.73145]\n",
      "[epoch 7], [iter 560 / 1121], [train loss 0.68152], [train acc 0.73516]\n",
      "[epoch 7], [iter 672 / 1121], [train loss 0.68022], [train acc 0.73582]\n",
      "[epoch 7], [iter 784 / 1121], [train loss 0.68114], [train acc 0.73593]\n",
      "[epoch 7], [iter 896 / 1121], [train loss 0.68105], [train acc 0.73626]\n",
      "[epoch 7], [iter 1008 / 1121], [train loss 0.67987], [train acc 0.73692]\n",
      "[epoch 7], [iter 1120 / 1121], [train loss 0.67701], [train acc 0.73834]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 2.05576], [val acc 0.36240]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 8\n",
      "[epoch 8], [iter 112 / 1121], [train loss 0.62291], [train acc 0.76786]\n",
      "[epoch 8], [iter 224 / 1121], [train loss 0.58622], [train acc 0.78013]\n",
      "[epoch 8], [iter 336 / 1121], [train loss 0.58371], [train acc 0.77976]\n",
      "[epoch 8], [iter 448 / 1121], [train loss 0.58884], [train acc 0.77693]\n",
      "[epoch 8], [iter 560 / 1121], [train loss 0.58855], [train acc 0.77673]\n",
      "[epoch 8], [iter 672 / 1121], [train loss 0.58485], [train acc 0.77823]\n",
      "[epoch 8], [iter 784 / 1121], [train loss 0.58209], [train acc 0.77910]\n",
      "[epoch 8], [iter 896 / 1121], [train loss 0.58035], [train acc 0.77940]\n",
      "[epoch 8], [iter 1008 / 1121], [train loss 0.57638], [train acc 0.78122]\n",
      "[epoch 8], [iter 1120 / 1121], [train loss 0.57189], [train acc 0.78292]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 2.09933], [val acc 0.38054]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 9\n",
      "[epoch 9], [iter 112 / 1121], [train loss 0.47471], [train acc 0.82394]\n",
      "[epoch 9], [iter 224 / 1121], [train loss 0.46790], [train acc 0.82520]\n",
      "[epoch 9], [iter 336 / 1121], [train loss 0.47632], [train acc 0.82013]\n",
      "[epoch 9], [iter 448 / 1121], [train loss 0.48020], [train acc 0.81961]\n",
      "[epoch 9], [iter 560 / 1121], [train loss 0.47892], [train acc 0.82070]\n",
      "[epoch 9], [iter 672 / 1121], [train loss 0.47763], [train acc 0.82101]\n",
      "[epoch 9], [iter 784 / 1121], [train loss 0.47798], [train acc 0.81995]\n",
      "[epoch 9], [iter 896 / 1121], [train loss 0.47821], [train acc 0.81920]\n",
      "[epoch 9], [iter 1008 / 1121], [train loss 0.48095], [train acc 0.81808]\n",
      "[epoch 9], [iter 1120 / 1121], [train loss 0.47893], [train acc 0.81914]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 3.20869], [val acc 0.16230]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 10\n",
      "[epoch 10], [iter 112 / 1121], [train loss 0.37619], [train acc 0.86719]\n",
      "[epoch 10], [iter 224 / 1121], [train loss 0.41535], [train acc 0.85142]\n",
      "[epoch 10], [iter 336 / 1121], [train loss 0.41942], [train acc 0.84784]\n",
      "[epoch 10], [iter 448 / 1121], [train loss 0.40886], [train acc 0.84961]\n",
      "[epoch 10], [iter 560 / 1121], [train loss 0.40884], [train acc 0.84872]\n",
      "[epoch 10], [iter 672 / 1121], [train loss 0.40581], [train acc 0.85003]\n",
      "[epoch 10], [iter 784 / 1121], [train loss 0.40466], [train acc 0.85069]\n",
      "[epoch 10], [iter 896 / 1121], [train loss 0.40105], [train acc 0.85114]\n",
      "[epoch 10], [iter 1008 / 1121], [train loss 0.40218], [train acc 0.85076]\n",
      "[epoch 10], [iter 1120 / 1121], [train loss 0.40358], [train acc 0.84927]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 2.47507], [val acc 0.47026]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 11\n",
      "[epoch 11], [iter 112 / 1121], [train loss 0.33258], [train acc 0.87612]\n",
      "[epoch 11], [iter 224 / 1121], [train loss 0.31876], [train acc 0.88546]\n",
      "[epoch 11], [iter 336 / 1121], [train loss 0.32371], [train acc 0.88170]\n",
      "[epoch 11], [iter 448 / 1121], [train loss 0.33037], [train acc 0.87828]\n",
      "[epoch 11], [iter 560 / 1121], [train loss 0.33806], [train acc 0.87656]\n",
      "[epoch 11], [iter 672 / 1121], [train loss 0.33425], [train acc 0.87821]\n",
      "[epoch 11], [iter 784 / 1121], [train loss 0.33099], [train acc 0.87974]\n",
      "[epoch 11], [iter 896 / 1121], [train loss 0.33217], [train acc 0.87877]\n",
      "[epoch 11], [iter 1008 / 1121], [train loss 0.33312], [train acc 0.87760]\n",
      "[epoch 11], [iter 1120 / 1121], [train loss 0.33436], [train acc 0.87734]\n",
      "------------------------------------------------------------\n",
      "[epoch 11], [val loss 2.91484], [val acc 0.29335]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 12\n",
      "[epoch 12], [iter 112 / 1121], [train loss 0.28328], [train acc 0.89537]\n",
      "[epoch 12], [iter 224 / 1121], [train loss 0.26296], [train acc 0.90360]\n",
      "[epoch 12], [iter 336 / 1121], [train loss 0.25231], [train acc 0.90718]\n",
      "[epoch 12], [iter 448 / 1121], [train loss 0.24842], [train acc 0.90834]\n",
      "[epoch 12], [iter 560 / 1121], [train loss 0.25339], [train acc 0.90642]\n",
      "[epoch 12], [iter 672 / 1121], [train loss 0.26631], [train acc 0.90211]\n",
      "[epoch 12], [iter 784 / 1121], [train loss 0.27244], [train acc 0.90063]\n",
      "[epoch 12], [iter 896 / 1121], [train loss 0.27692], [train acc 0.89907]\n",
      "[epoch 12], [iter 1008 / 1121], [train loss 0.28193], [train acc 0.89726]\n",
      "[epoch 12], [iter 1120 / 1121], [train loss 0.28365], [train acc 0.89651]\n",
      "------------------------------------------------------------\n",
      "[epoch 12], [val loss 2.64335], [val acc 0.49042]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 13\n",
      "[epoch 13], [iter 112 / 1121], [train loss 0.24260], [train acc 0.91378]\n",
      "[epoch 13], [iter 224 / 1121], [train loss 0.25834], [train acc 0.90862]\n",
      "[epoch 13], [iter 336 / 1121], [train loss 0.25535], [train acc 0.91025]\n",
      "[epoch 13], [iter 448 / 1121], [train loss 0.27117], [train acc 0.90276]\n",
      "[epoch 13], [iter 560 / 1121], [train loss 0.26368], [train acc 0.90513]\n",
      "[epoch 13], [iter 672 / 1121], [train loss 0.25830], [train acc 0.90634]\n",
      "[epoch 13], [iter 784 / 1121], [train loss 0.26181], [train acc 0.90493]\n",
      "[epoch 13], [iter 896 / 1121], [train loss 0.25873], [train acc 0.90604]\n",
      "[epoch 13], [iter 1008 / 1121], [train loss 0.25865], [train acc 0.90532]\n",
      "[epoch 13], [iter 1120 / 1121], [train loss 0.26071], [train acc 0.90508]\n",
      "------------------------------------------------------------\n",
      "[epoch 13], [val loss 2.96133], [val acc 0.40071]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 14\n",
      "[epoch 14], [iter 112 / 1121], [train loss 0.28760], [train acc 0.90234]\n",
      "[epoch 14], [iter 224 / 1121], [train loss 0.24117], [train acc 0.91560]\n",
      "[epoch 14], [iter 336 / 1121], [train loss 0.22686], [train acc 0.91908]\n",
      "[epoch 14], [iter 448 / 1121], [train loss 0.22474], [train acc 0.92013]\n",
      "[epoch 14], [iter 560 / 1121], [train loss 0.22304], [train acc 0.92065]\n",
      "[epoch 14], [iter 672 / 1121], [train loss 0.22736], [train acc 0.91820]\n",
      "[epoch 14], [iter 784 / 1121], [train loss 0.22478], [train acc 0.91901]\n",
      "[epoch 14], [iter 896 / 1121], [train loss 0.22762], [train acc 0.91783]\n",
      "[epoch 14], [iter 1008 / 1121], [train loss 0.22627], [train acc 0.91815]\n",
      "[epoch 14], [iter 1120 / 1121], [train loss 0.22508], [train acc 0.91867]\n",
      "------------------------------------------------------------\n",
      "[epoch 14], [val loss 2.96864], [val acc 0.50302]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 15\n",
      "[epoch 15], [iter 112 / 1121], [train loss 0.14105], [train acc 0.95117]\n",
      "[epoch 15], [iter 224 / 1121], [train loss 0.14362], [train acc 0.95061]\n",
      "[epoch 15], [iter 336 / 1121], [train loss 0.16644], [train acc 0.94243]\n",
      "[epoch 15], [iter 448 / 1121], [train loss 0.17094], [train acc 0.94043]\n",
      "[epoch 15], [iter 560 / 1121], [train loss 0.17232], [train acc 0.93945]\n",
      "[epoch 15], [iter 672 / 1121], [train loss 0.17195], [train acc 0.93852]\n",
      "[epoch 15], [iter 784 / 1121], [train loss 0.17657], [train acc 0.93734]\n",
      "[epoch 15], [iter 896 / 1121], [train loss 0.17874], [train acc 0.93691]\n",
      "[epoch 15], [iter 1008 / 1121], [train loss 0.18056], [train acc 0.93623]\n",
      "[epoch 15], [iter 1120 / 1121], [train loss 0.18400], [train acc 0.93482]\n",
      "------------------------------------------------------------\n",
      "[epoch 15], [val loss 3.26576], [val acc 0.47278]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 16\n",
      "[epoch 16], [iter 112 / 1121], [train loss 0.29953], [train acc 0.89286]\n",
      "[epoch 16], [iter 224 / 1121], [train loss 0.25719], [train acc 0.90932]\n",
      "[epoch 16], [iter 336 / 1121], [train loss 0.22624], [train acc 0.92039]\n",
      "[epoch 16], [iter 448 / 1121], [train loss 0.21012], [train acc 0.92662]\n",
      "[epoch 16], [iter 560 / 1121], [train loss 0.19898], [train acc 0.93080]\n",
      "[epoch 16], [iter 672 / 1121], [train loss 0.19107], [train acc 0.93350]\n",
      "[epoch 16], [iter 784 / 1121], [train loss 0.19199], [train acc 0.93284]\n",
      "[epoch 16], [iter 896 / 1121], [train loss 0.19046], [train acc 0.93384]\n",
      "[epoch 16], [iter 1008 / 1121], [train loss 0.18928], [train acc 0.93344]\n",
      "[epoch 16], [iter 1120 / 1121], [train loss 0.18638], [train acc 0.93415]\n",
      "------------------------------------------------------------\n",
      "[epoch 16], [val loss 3.48030], [val acc 0.29032]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 17\n",
      "[epoch 17], [iter 112 / 1121], [train loss 0.14163], [train acc 0.95117]\n",
      "[epoch 17], [iter 224 / 1121], [train loss 0.13231], [train acc 0.95564]\n",
      "[epoch 17], [iter 336 / 1121], [train loss 0.12593], [train acc 0.95638]\n",
      "[epoch 17], [iter 448 / 1121], [train loss 0.12810], [train acc 0.95668]\n",
      "[epoch 17], [iter 560 / 1121], [train loss 0.13254], [train acc 0.95502]\n",
      "[epoch 17], [iter 672 / 1121], [train loss 0.13560], [train acc 0.95340]\n",
      "[epoch 17], [iter 784 / 1121], [train loss 0.14166], [train acc 0.95101]\n",
      "[epoch 17], [iter 896 / 1121], [train loss 0.14452], [train acc 0.94971]\n",
      "[epoch 17], [iter 1008 / 1121], [train loss 0.14781], [train acc 0.94847]\n",
      "[epoch 17], [iter 1120 / 1121], [train loss 0.14964], [train acc 0.94774]\n",
      "------------------------------------------------------------\n",
      "[epoch 17], [val loss 3.32071], [val acc 0.36744]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 18\n",
      "[epoch 18], [iter 112 / 1121], [train loss 0.16443], [train acc 0.94224]\n",
      "[epoch 18], [iter 224 / 1121], [train loss 0.14998], [train acc 0.94894]\n",
      "[epoch 18], [iter 336 / 1121], [train loss 0.13560], [train acc 0.95312]\n",
      "[epoch 18], [iter 448 / 1121], [train loss 0.13388], [train acc 0.95403]\n",
      "[epoch 18], [iter 560 / 1121], [train loss 0.13687], [train acc 0.95268]\n",
      "[epoch 18], [iter 672 / 1121], [train loss 0.14083], [train acc 0.95066]\n",
      "[epoch 18], [iter 784 / 1121], [train loss 0.14343], [train acc 0.94986]\n",
      "[epoch 18], [iter 896 / 1121], [train loss 0.14695], [train acc 0.94845]\n",
      "[epoch 18], [iter 1008 / 1121], [train loss 0.14977], [train acc 0.94785]\n",
      "[epoch 18], [iter 1120 / 1121], [train loss 0.15243], [train acc 0.94696]\n",
      "------------------------------------------------------------\n",
      "[epoch 18], [val loss 3.53319], [val acc 0.28276]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 19\n",
      "[epoch 19], [iter 112 / 1121], [train loss 0.12534], [train acc 0.95480]\n",
      "[epoch 19], [iter 224 / 1121], [train loss 0.11704], [train acc 0.96010]\n",
      "[epoch 19], [iter 336 / 1121], [train loss 0.11029], [train acc 0.96252]\n",
      "[epoch 19], [iter 448 / 1121], [train loss 0.11712], [train acc 0.95940]\n",
      "[epoch 19], [iter 560 / 1121], [train loss 0.12366], [train acc 0.95686]\n",
      "[epoch 19], [iter 672 / 1121], [train loss 0.12664], [train acc 0.95573]\n",
      "[epoch 19], [iter 784 / 1121], [train loss 0.12817], [train acc 0.95476]\n",
      "[epoch 19], [iter 896 / 1121], [train loss 0.13344], [train acc 0.95295]\n",
      "[epoch 19], [iter 1008 / 1121], [train loss 0.13698], [train acc 0.95108]\n",
      "[epoch 19], [iter 1120 / 1121], [train loss 0.13742], [train acc 0.95098]\n",
      "------------------------------------------------------------\n",
      "[epoch 19], [val loss 3.63868], [val acc 0.33972]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 20\n",
      "[epoch 20], [iter 112 / 1121], [train loss 0.14443], [train acc 0.95006]\n",
      "[epoch 20], [iter 224 / 1121], [train loss 0.14527], [train acc 0.94880]\n",
      "[epoch 20], [iter 336 / 1121], [train loss 0.13184], [train acc 0.95322]\n",
      "[epoch 20], [iter 448 / 1121], [train loss 0.12463], [train acc 0.95654]\n",
      "[epoch 20], [iter 560 / 1121], [train loss 0.12340], [train acc 0.95759]\n",
      "[epoch 20], [iter 672 / 1121], [train loss 0.12505], [train acc 0.95615]\n",
      "[epoch 20], [iter 784 / 1121], [train loss 0.12274], [train acc 0.95699]\n",
      "[epoch 20], [iter 896 / 1121], [train loss 0.12425], [train acc 0.95651]\n",
      "[epoch 20], [iter 1008 / 1121], [train loss 0.12373], [train acc 0.95660]\n",
      "[epoch 20], [iter 1120 / 1121], [train loss 0.12587], [train acc 0.95594]\n",
      "------------------------------------------------------------\n",
      "[epoch 20], [val loss 4.41106], [val acc 0.27621]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 21\n",
      "[epoch 21], [iter 112 / 1121], [train loss 0.17642], [train acc 0.93722]\n",
      "[epoch 21], [iter 224 / 1121], [train loss 0.14337], [train acc 0.95047]\n",
      "[epoch 21], [iter 336 / 1121], [train loss 0.13274], [train acc 0.95452]\n",
      "[epoch 21], [iter 448 / 1121], [train loss 0.12309], [train acc 0.95773]\n",
      "[epoch 21], [iter 560 / 1121], [train loss 0.12304], [train acc 0.95820]\n",
      "[epoch 21], [iter 672 / 1121], [train loss 0.12224], [train acc 0.95824]\n",
      "[epoch 21], [iter 784 / 1121], [train loss 0.12492], [train acc 0.95671]\n",
      "[epoch 21], [iter 896 / 1121], [train loss 0.12570], [train acc 0.95651]\n",
      "[epoch 21], [iter 1008 / 1121], [train loss 0.12351], [train acc 0.95728]\n",
      "[epoch 21], [iter 1120 / 1121], [train loss 0.12322], [train acc 0.95723]\n",
      "------------------------------------------------------------\n",
      "[epoch 21], [val loss 3.64232], [val acc 0.44204]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 22\n",
      "[epoch 22], [iter 112 / 1121], [train loss 0.13696], [train acc 0.95173]\n",
      "[epoch 22], [iter 224 / 1121], [train loss 0.12551], [train acc 0.95424]\n",
      "[epoch 22], [iter 336 / 1121], [train loss 0.11177], [train acc 0.95954]\n",
      "[epoch 22], [iter 448 / 1121], [train loss 0.10778], [train acc 0.96122]\n",
      "[epoch 22], [iter 560 / 1121], [train loss 0.10362], [train acc 0.96289]\n",
      "[epoch 22], [iter 672 / 1121], [train loss 0.10189], [train acc 0.96303]\n",
      "[epoch 22], [iter 784 / 1121], [train loss 0.10444], [train acc 0.96165]\n",
      "[epoch 22], [iter 896 / 1121], [train loss 0.11150], [train acc 0.95972]\n",
      "[epoch 22], [iter 1008 / 1121], [train loss 0.11161], [train acc 0.95929]\n",
      "[epoch 22], [iter 1120 / 1121], [train loss 0.11111], [train acc 0.95960]\n",
      "------------------------------------------------------------\n",
      "[epoch 22], [val loss 3.71725], [val acc 0.35081]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 23\n",
      "[epoch 23], [iter 112 / 1121], [train loss 0.23022], [train acc 0.92020]\n",
      "[epoch 23], [iter 224 / 1121], [train loss 0.16881], [train acc 0.94071]\n",
      "[epoch 23], [iter 336 / 1121], [train loss 0.14243], [train acc 0.95071]\n",
      "[epoch 23], [iter 448 / 1121], [train loss 0.12790], [train acc 0.95661]\n",
      "[epoch 23], [iter 560 / 1121], [train loss 0.11899], [train acc 0.95915]\n",
      "[epoch 23], [iter 672 / 1121], [train loss 0.11408], [train acc 0.96150]\n",
      "[epoch 23], [iter 784 / 1121], [train loss 0.11024], [train acc 0.96237]\n",
      "[epoch 23], [iter 896 / 1121], [train loss 0.11280], [train acc 0.96209]\n",
      "[epoch 23], [iter 1008 / 1121], [train loss 0.11630], [train acc 0.96100]\n",
      "[epoch 23], [iter 1120 / 1121], [train loss 0.11804], [train acc 0.95996]\n",
      "------------------------------------------------------------\n",
      "[epoch 23], [val loss 3.48325], [val acc 0.43347]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 24\n",
      "[epoch 24], [iter 112 / 1121], [train loss 0.12152], [train acc 0.95898]\n",
      "[epoch 24], [iter 224 / 1121], [train loss 0.10032], [train acc 0.96526]\n",
      "[epoch 24], [iter 336 / 1121], [train loss 0.09587], [train acc 0.96642]\n",
      "[epoch 24], [iter 448 / 1121], [train loss 0.09844], [train acc 0.96610]\n",
      "[epoch 24], [iter 560 / 1121], [train loss 0.09727], [train acc 0.96708]\n",
      "[epoch 24], [iter 672 / 1121], [train loss 0.09694], [train acc 0.96749]\n",
      "[epoch 24], [iter 784 / 1121], [train loss 0.09324], [train acc 0.96879]\n",
      "[epoch 24], [iter 896 / 1121], [train loss 0.09199], [train acc 0.96938]\n",
      "[epoch 24], [iter 1008 / 1121], [train loss 0.09545], [train acc 0.96804]\n",
      "[epoch 24], [iter 1120 / 1121], [train loss 0.09825], [train acc 0.96688]\n",
      "------------------------------------------------------------\n",
      "[epoch 24], [val loss 3.58648], [val acc 0.43700]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 25\n",
      "[epoch 25], [iter 112 / 1121], [train loss 0.09860], [train acc 0.96959]\n",
      "[epoch 25], [iter 224 / 1121], [train loss 0.09216], [train acc 0.97042]\n",
      "[epoch 25], [iter 336 / 1121], [train loss 0.08779], [train acc 0.97089]\n",
      "[epoch 25], [iter 448 / 1121], [train loss 0.08742], [train acc 0.97126]\n",
      "[epoch 25], [iter 560 / 1121], [train loss 0.09198], [train acc 0.96953]\n",
      "[epoch 25], [iter 672 / 1121], [train loss 0.09260], [train acc 0.96875]\n",
      "[epoch 25], [iter 784 / 1121], [train loss 0.09124], [train acc 0.96907]\n",
      "[epoch 25], [iter 896 / 1121], [train loss 0.09068], [train acc 0.96910]\n",
      "[epoch 25], [iter 1008 / 1121], [train loss 0.09219], [train acc 0.96850]\n",
      "[epoch 25], [iter 1120 / 1121], [train loss 0.09373], [train acc 0.96825]\n",
      "------------------------------------------------------------\n",
      "[epoch 25], [val loss 4.07582], [val acc 0.40776]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 26\n",
      "[epoch 26], [iter 112 / 1121], [train loss 0.08575], [train acc 0.97098]\n",
      "[epoch 26], [iter 224 / 1121], [train loss 0.07962], [train acc 0.97377]\n",
      "[epoch 26], [iter 336 / 1121], [train loss 0.07879], [train acc 0.97433]\n",
      "[epoch 26], [iter 448 / 1121], [train loss 0.07627], [train acc 0.97538]\n",
      "[epoch 26], [iter 560 / 1121], [train loss 0.07406], [train acc 0.97573]\n",
      "[epoch 26], [iter 672 / 1121], [train loss 0.07784], [train acc 0.97414]\n",
      "[epoch 26], [iter 784 / 1121], [train loss 0.07827], [train acc 0.97393]\n",
      "[epoch 26], [iter 896 / 1121], [train loss 0.07997], [train acc 0.97297]\n",
      "[epoch 26], [iter 1008 / 1121], [train loss 0.08314], [train acc 0.97166]\n",
      "[epoch 26], [iter 1120 / 1121], [train loss 0.08752], [train acc 0.97012]\n",
      "------------------------------------------------------------\n",
      "[epoch 26], [val loss 3.91959], [val acc 0.44506]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 27\n",
      "[epoch 27], [iter 112 / 1121], [train loss 0.13725], [train acc 0.95396]\n",
      "[epoch 27], [iter 224 / 1121], [train loss 0.11020], [train acc 0.96094]\n",
      "[epoch 27], [iter 336 / 1121], [train loss 0.09788], [train acc 0.96587]\n",
      "[epoch 27], [iter 448 / 1121], [train loss 0.09043], [train acc 0.96910]\n",
      "[epoch 27], [iter 560 / 1121], [train loss 0.09121], [train acc 0.96858]\n",
      "[epoch 27], [iter 672 / 1121], [train loss 0.09230], [train acc 0.96828]\n",
      "[epoch 27], [iter 784 / 1121], [train loss 0.09873], [train acc 0.96604]\n",
      "[epoch 27], [iter 896 / 1121], [train loss 0.10581], [train acc 0.96362]\n",
      "[epoch 27], [iter 1008 / 1121], [train loss 0.11125], [train acc 0.96196]\n",
      "[epoch 27], [iter 1120 / 1121], [train loss 0.10921], [train acc 0.96233]\n",
      "------------------------------------------------------------\n",
      "[epoch 27], [val loss 4.10685], [val acc 0.40827]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 28\n",
      "[epoch 28], [iter 112 / 1121], [train loss 0.38313], [train acc 0.87556]\n",
      "[epoch 28], [iter 224 / 1121], [train loss 0.26117], [train acc 0.91546]\n",
      "[epoch 28], [iter 336 / 1121], [train loss 0.19915], [train acc 0.93527]\n",
      "[epoch 28], [iter 448 / 1121], [train loss 0.16688], [train acc 0.94594]\n",
      "[epoch 28], [iter 560 / 1121], [train loss 0.14549], [train acc 0.95229]\n",
      "[epoch 28], [iter 672 / 1121], [train loss 0.13325], [train acc 0.95633]\n",
      "[epoch 28], [iter 784 / 1121], [train loss 0.12346], [train acc 0.95938]\n",
      "[epoch 28], [iter 896 / 1121], [train loss 0.11679], [train acc 0.96157]\n",
      "[epoch 28], [iter 1008 / 1121], [train loss 0.11174], [train acc 0.96323]\n",
      "[epoch 28], [iter 1120 / 1121], [train loss 0.10694], [train acc 0.96468]\n",
      "------------------------------------------------------------\n",
      "[epoch 28], [val loss 3.83696], [val acc 0.47530]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 29\n",
      "[epoch 29], [iter 112 / 1121], [train loss 0.08617], [train acc 0.96931]\n",
      "[epoch 29], [iter 224 / 1121], [train loss 0.07089], [train acc 0.97545]\n",
      "[epoch 29], [iter 336 / 1121], [train loss 0.07194], [train acc 0.97489]\n",
      "[epoch 29], [iter 448 / 1121], [train loss 0.06571], [train acc 0.97691]\n",
      "[epoch 29], [iter 560 / 1121], [train loss 0.06608], [train acc 0.97679]\n",
      "[epoch 29], [iter 672 / 1121], [train loss 0.06280], [train acc 0.97791]\n",
      "[epoch 29], [iter 784 / 1121], [train loss 0.06243], [train acc 0.97796]\n",
      "[epoch 29], [iter 896 / 1121], [train loss 0.06303], [train acc 0.97775]\n",
      "[epoch 29], [iter 1008 / 1121], [train loss 0.06376], [train acc 0.97724]\n",
      "[epoch 29], [iter 1120 / 1121], [train loss 0.06742], [train acc 0.97614]\n",
      "------------------------------------------------------------\n",
      "[epoch 29], [val loss 4.11859], [val acc 0.42893]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 30\n",
      "[epoch 30], [iter 112 / 1121], [train loss 0.08043], [train acc 0.97098]\n",
      "[epoch 30], [iter 224 / 1121], [train loss 0.08325], [train acc 0.97196]\n",
      "[epoch 30], [iter 336 / 1121], [train loss 0.08327], [train acc 0.97126]\n",
      "[epoch 30], [iter 448 / 1121], [train loss 0.08413], [train acc 0.97098]\n",
      "[epoch 30], [iter 560 / 1121], [train loss 0.08353], [train acc 0.97148]\n",
      "[epoch 30], [iter 672 / 1121], [train loss 0.08113], [train acc 0.97252]\n",
      "[epoch 30], [iter 784 / 1121], [train loss 0.07811], [train acc 0.97345]\n",
      "[epoch 30], [iter 896 / 1121], [train loss 0.07986], [train acc 0.97339]\n",
      "[epoch 30], [iter 1008 / 1121], [train loss 0.08269], [train acc 0.97232]\n",
      "[epoch 30], [iter 1120 / 1121], [train loss 0.08367], [train acc 0.97185]\n",
      "------------------------------------------------------------\n",
      "[epoch 30], [val loss 3.90994], [val acc 0.45766]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 31\n",
      "[epoch 31], [iter 112 / 1121], [train loss 0.08819], [train acc 0.97461]\n",
      "[epoch 31], [iter 224 / 1121], [train loss 0.07687], [train acc 0.97754]\n",
      "[epoch 31], [iter 336 / 1121], [train loss 0.07695], [train acc 0.97712]\n",
      "[epoch 31], [iter 448 / 1121], [train loss 0.07355], [train acc 0.97733]\n",
      "[epoch 31], [iter 560 / 1121], [train loss 0.07182], [train acc 0.97757]\n",
      "[epoch 31], [iter 672 / 1121], [train loss 0.07238], [train acc 0.97689]\n",
      "[epoch 31], [iter 784 / 1121], [train loss 0.07451], [train acc 0.97565]\n",
      "[epoch 31], [iter 896 / 1121], [train loss 0.07469], [train acc 0.97552]\n",
      "[epoch 31], [iter 1008 / 1121], [train loss 0.07775], [train acc 0.97445]\n",
      "[epoch 31], [iter 1120 / 1121], [train loss 0.07824], [train acc 0.97400]\n",
      "------------------------------------------------------------\n",
      "[epoch 31], [val loss 4.08410], [val acc 0.44002]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 32\n",
      "[epoch 32], [iter 112 / 1121], [train loss 0.20149], [train acc 0.93136]\n",
      "[epoch 32], [iter 224 / 1121], [train loss 0.14511], [train acc 0.95187]\n",
      "[epoch 32], [iter 336 / 1121], [train loss 0.11868], [train acc 0.96001]\n",
      "[epoch 32], [iter 448 / 1121], [train loss 0.10158], [train acc 0.96603]\n",
      "[epoch 32], [iter 560 / 1121], [train loss 0.08877], [train acc 0.97037]\n",
      "[epoch 32], [iter 672 / 1121], [train loss 0.08195], [train acc 0.97242]\n",
      "[epoch 32], [iter 784 / 1121], [train loss 0.07776], [train acc 0.97353]\n",
      "[epoch 32], [iter 896 / 1121], [train loss 0.07532], [train acc 0.97395]\n",
      "[epoch 32], [iter 1008 / 1121], [train loss 0.07431], [train acc 0.97445]\n",
      "[epoch 32], [iter 1120 / 1121], [train loss 0.07312], [train acc 0.97483]\n",
      "------------------------------------------------------------\n",
      "[epoch 32], [val loss 3.98988], [val acc 0.49698]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 33\n",
      "[epoch 33], [iter 112 / 1121], [train loss 0.10228], [train acc 0.96484]\n",
      "[epoch 33], [iter 224 / 1121], [train loss 0.08821], [train acc 0.97015]\n",
      "[epoch 33], [iter 336 / 1121], [train loss 0.08159], [train acc 0.97228]\n",
      "[epoch 33], [iter 448 / 1121], [train loss 0.08064], [train acc 0.97314]\n",
      "[epoch 33], [iter 560 / 1121], [train loss 0.07576], [train acc 0.97450]\n",
      "[epoch 33], [iter 672 / 1121], [train loss 0.07293], [train acc 0.97554]\n",
      "[epoch 33], [iter 784 / 1121], [train loss 0.07029], [train acc 0.97660]\n",
      "[epoch 33], [iter 896 / 1121], [train loss 0.07188], [train acc 0.97593]\n",
      "[epoch 33], [iter 1008 / 1121], [train loss 0.07302], [train acc 0.97548]\n",
      "[epoch 33], [iter 1120 / 1121], [train loss 0.07404], [train acc 0.97494]\n",
      "------------------------------------------------------------\n",
      "[epoch 33], [val loss 4.34062], [val acc 0.47681]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 34\n",
      "[epoch 34], [iter 112 / 1121], [train loss 0.09848], [train acc 0.96568]\n",
      "[epoch 34], [iter 224 / 1121], [train loss 0.07426], [train acc 0.97475]\n",
      "[epoch 34], [iter 336 / 1121], [train loss 0.07689], [train acc 0.97424]\n",
      "[epoch 34], [iter 448 / 1121], [train loss 0.07796], [train acc 0.97287]\n",
      "[epoch 34], [iter 560 / 1121], [train loss 0.08577], [train acc 0.97042]\n",
      "[epoch 34], [iter 672 / 1121], [train loss 0.08386], [train acc 0.97131]\n",
      "[epoch 34], [iter 784 / 1121], [train loss 0.07942], [train acc 0.97298]\n",
      "[epoch 34], [iter 896 / 1121], [train loss 0.07464], [train acc 0.97471]\n",
      "[epoch 34], [iter 1008 / 1121], [train loss 0.07341], [train acc 0.97501]\n",
      "[epoch 34], [iter 1120 / 1121], [train loss 0.07321], [train acc 0.97503]\n",
      "------------------------------------------------------------\n",
      "[epoch 34], [val loss 4.39560], [val acc 0.40020]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 35\n",
      "[epoch 35], [iter 112 / 1121], [train loss 0.07591], [train acc 0.97405]\n",
      "[epoch 35], [iter 224 / 1121], [train loss 0.07014], [train acc 0.97475]\n",
      "[epoch 35], [iter 336 / 1121], [train loss 0.07727], [train acc 0.97452]\n",
      "[epoch 35], [iter 448 / 1121], [train loss 0.07660], [train acc 0.97440]\n",
      "[epoch 35], [iter 560 / 1121], [train loss 0.07124], [train acc 0.97628]\n",
      "[epoch 35], [iter 672 / 1121], [train loss 0.06542], [train acc 0.97824]\n",
      "[epoch 35], [iter 784 / 1121], [train loss 0.06405], [train acc 0.97875]\n",
      "[epoch 35], [iter 896 / 1121], [train loss 0.06429], [train acc 0.97845]\n",
      "[epoch 35], [iter 1008 / 1121], [train loss 0.06440], [train acc 0.97821]\n",
      "[epoch 35], [iter 1120 / 1121], [train loss 0.06475], [train acc 0.97796]\n",
      "------------------------------------------------------------\n",
      "[epoch 35], [val loss 4.36020], [val acc 0.44304]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 36\n",
      "[epoch 36], [iter 112 / 1121], [train loss 0.08971], [train acc 0.96903]\n",
      "[epoch 36], [iter 224 / 1121], [train loss 0.08358], [train acc 0.97112]\n",
      "[epoch 36], [iter 336 / 1121], [train loss 0.07288], [train acc 0.97377]\n",
      "[epoch 36], [iter 448 / 1121], [train loss 0.06641], [train acc 0.97621]\n",
      "[epoch 36], [iter 560 / 1121], [train loss 0.06285], [train acc 0.97734]\n",
      "[epoch 36], [iter 672 / 1121], [train loss 0.06264], [train acc 0.97707]\n",
      "[epoch 36], [iter 784 / 1121], [train loss 0.06130], [train acc 0.97772]\n",
      "[epoch 36], [iter 896 / 1121], [train loss 0.06077], [train acc 0.97813]\n",
      "[epoch 36], [iter 1008 / 1121], [train loss 0.06187], [train acc 0.97752]\n",
      "[epoch 36], [iter 1120 / 1121], [train loss 0.06232], [train acc 0.97734]\n",
      "------------------------------------------------------------\n",
      "[epoch 36], [val loss 4.56434], [val acc 0.38256]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 37\n",
      "[epoch 37], [iter 112 / 1121], [train loss 0.12499], [train acc 0.96038]\n",
      "[epoch 37], [iter 224 / 1121], [train loss 0.09212], [train acc 0.96987]\n",
      "[epoch 37], [iter 336 / 1121], [train loss 0.07933], [train acc 0.97340]\n",
      "[epoch 37], [iter 448 / 1121], [train loss 0.07425], [train acc 0.97433]\n",
      "[epoch 37], [iter 560 / 1121], [train loss 0.06912], [train acc 0.97561]\n",
      "[epoch 37], [iter 672 / 1121], [train loss 0.06486], [train acc 0.97726]\n",
      "[epoch 37], [iter 784 / 1121], [train loss 0.06370], [train acc 0.97768]\n",
      "[epoch 37], [iter 896 / 1121], [train loss 0.06552], [train acc 0.97733]\n",
      "[epoch 37], [iter 1008 / 1121], [train loss 0.06598], [train acc 0.97724]\n",
      "[epoch 37], [iter 1120 / 1121], [train loss 0.06492], [train acc 0.97762]\n",
      "------------------------------------------------------------\n",
      "[epoch 37], [val loss 4.37912], [val acc 0.44103]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 38\n",
      "[epoch 38], [iter 112 / 1121], [train loss 0.20026], [train acc 0.93359]\n",
      "[epoch 38], [iter 224 / 1121], [train loss 0.14266], [train acc 0.95201]\n",
      "[epoch 38], [iter 336 / 1121], [train loss 0.11584], [train acc 0.96150]\n",
      "[epoch 38], [iter 448 / 1121], [train loss 0.10076], [train acc 0.96617]\n",
      "[epoch 38], [iter 560 / 1121], [train loss 0.09202], [train acc 0.96903]\n",
      "[epoch 38], [iter 672 / 1121], [train loss 0.08470], [train acc 0.97117]\n",
      "[epoch 38], [iter 784 / 1121], [train loss 0.07863], [train acc 0.97329]\n",
      "[epoch 38], [iter 896 / 1121], [train loss 0.07578], [train acc 0.97426]\n",
      "[epoch 38], [iter 1008 / 1121], [train loss 0.07251], [train acc 0.97526]\n",
      "[epoch 38], [iter 1120 / 1121], [train loss 0.07177], [train acc 0.97547]\n",
      "------------------------------------------------------------\n",
      "[epoch 38], [val loss 4.73935], [val acc 0.34627]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 39\n",
      "[epoch 39], [iter 112 / 1121], [train loss 0.06796], [train acc 0.97600]\n",
      "[epoch 39], [iter 224 / 1121], [train loss 0.05384], [train acc 0.98145]\n",
      "[epoch 39], [iter 336 / 1121], [train loss 0.05413], [train acc 0.98168]\n",
      "[epoch 39], [iter 448 / 1121], [train loss 0.05063], [train acc 0.98235]\n",
      "[epoch 39], [iter 560 / 1121], [train loss 0.04752], [train acc 0.98331]\n",
      "[epoch 39], [iter 672 / 1121], [train loss 0.04560], [train acc 0.98424]\n",
      "[epoch 39], [iter 784 / 1121], [train loss 0.04618], [train acc 0.98394]\n",
      "[epoch 39], [iter 896 / 1121], [train loss 0.04798], [train acc 0.98347]\n",
      "[epoch 39], [iter 1008 / 1121], [train loss 0.05175], [train acc 0.98208]\n",
      "[epoch 39], [iter 1120 / 1121], [train loss 0.05405], [train acc 0.98125]\n",
      "------------------------------------------------------------\n",
      "[epoch 39], [val loss 4.36809], [val acc 0.49294]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 40\n",
      "[epoch 40], [iter 112 / 1121], [train loss 0.16431], [train acc 0.95145]\n",
      "[epoch 40], [iter 224 / 1121], [train loss 0.11290], [train acc 0.96582]\n",
      "[epoch 40], [iter 336 / 1121], [train loss 0.09026], [train acc 0.97210]\n",
      "[epoch 40], [iter 448 / 1121], [train loss 0.08133], [train acc 0.97461]\n",
      "[epoch 40], [iter 560 / 1121], [train loss 0.07486], [train acc 0.97617]\n",
      "[epoch 40], [iter 672 / 1121], [train loss 0.07240], [train acc 0.97680]\n",
      "[epoch 40], [iter 784 / 1121], [train loss 0.06993], [train acc 0.97768]\n",
      "[epoch 40], [iter 896 / 1121], [train loss 0.06680], [train acc 0.97859]\n",
      "[epoch 40], [iter 1008 / 1121], [train loss 0.06551], [train acc 0.97876]\n",
      "[epoch 40], [iter 1120 / 1121], [train loss 0.06312], [train acc 0.97944]\n",
      "------------------------------------------------------------\n",
      "[epoch 40], [val loss 4.15874], [val acc 0.50151]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 41\n",
      "[epoch 41], [iter 112 / 1121], [train loss 0.06357], [train acc 0.98075]\n",
      "[epoch 41], [iter 224 / 1121], [train loss 0.06079], [train acc 0.98256]\n",
      "[epoch 41], [iter 336 / 1121], [train loss 0.05729], [train acc 0.98214]\n",
      "[epoch 41], [iter 448 / 1121], [train loss 0.05570], [train acc 0.98221]\n",
      "[epoch 41], [iter 560 / 1121], [train loss 0.05559], [train acc 0.98170]\n",
      "[epoch 41], [iter 672 / 1121], [train loss 0.05385], [train acc 0.98247]\n",
      "[epoch 41], [iter 784 / 1121], [train loss 0.05124], [train acc 0.98342]\n",
      "[epoch 41], [iter 896 / 1121], [train loss 0.05117], [train acc 0.98354]\n",
      "[epoch 41], [iter 1008 / 1121], [train loss 0.05284], [train acc 0.98307]\n",
      "[epoch 41], [iter 1120 / 1121], [train loss 0.05338], [train acc 0.98290]\n",
      "------------------------------------------------------------\n",
      "[epoch 41], [val loss 4.99279], [val acc 0.35837]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 42\n",
      "[epoch 42], [iter 112 / 1121], [train loss 0.11726], [train acc 0.96038]\n",
      "[epoch 42], [iter 224 / 1121], [train loss 0.08371], [train acc 0.97182]\n",
      "[epoch 42], [iter 336 / 1121], [train loss 0.06821], [train acc 0.97712]\n",
      "[epoch 42], [iter 448 / 1121], [train loss 0.06267], [train acc 0.97859]\n",
      "[epoch 42], [iter 560 / 1121], [train loss 0.06099], [train acc 0.97913]\n",
      "[epoch 42], [iter 672 / 1121], [train loss 0.06254], [train acc 0.97847]\n",
      "[epoch 42], [iter 784 / 1121], [train loss 0.06077], [train acc 0.97915]\n",
      "[epoch 42], [iter 896 / 1121], [train loss 0.05881], [train acc 0.98002]\n",
      "[epoch 42], [iter 1008 / 1121], [train loss 0.05686], [train acc 0.98072]\n",
      "[epoch 42], [iter 1120 / 1121], [train loss 0.05452], [train acc 0.98156]\n",
      "------------------------------------------------------------\n",
      "[epoch 42], [val loss 4.21241], [val acc 0.45716]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 43\n",
      "[epoch 43], [iter 112 / 1121], [train loss 0.07304], [train acc 0.97740]\n",
      "[epoch 43], [iter 224 / 1121], [train loss 0.05691], [train acc 0.98172]\n",
      "[epoch 43], [iter 336 / 1121], [train loss 0.05039], [train acc 0.98326]\n",
      "[epoch 43], [iter 448 / 1121], [train loss 0.04721], [train acc 0.98417]\n",
      "[epoch 43], [iter 560 / 1121], [train loss 0.04664], [train acc 0.98443]\n",
      "[epoch 43], [iter 672 / 1121], [train loss 0.04887], [train acc 0.98372]\n",
      "[epoch 43], [iter 784 / 1121], [train loss 0.05033], [train acc 0.98334]\n",
      "[epoch 43], [iter 896 / 1121], [train loss 0.06377], [train acc 0.97935]\n",
      "[epoch 43], [iter 1008 / 1121], [train loss 0.06695], [train acc 0.97836]\n",
      "[epoch 43], [iter 1120 / 1121], [train loss 0.06663], [train acc 0.97813]\n",
      "------------------------------------------------------------\n",
      "[epoch 43], [val loss 4.37486], [val acc 0.37601]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 44\n",
      "[epoch 44], [iter 112 / 1121], [train loss 0.14672], [train acc 0.95898]\n",
      "[epoch 44], [iter 224 / 1121], [train loss 0.10781], [train acc 0.96735]\n",
      "[epoch 44], [iter 336 / 1121], [train loss 0.08729], [train acc 0.97359]\n",
      "[epoch 44], [iter 448 / 1121], [train loss 0.07389], [train acc 0.97761]\n",
      "[epoch 44], [iter 560 / 1121], [train loss 0.06504], [train acc 0.98025]\n",
      "[epoch 44], [iter 672 / 1121], [train loss 0.05930], [train acc 0.98177]\n",
      "[epoch 44], [iter 784 / 1121], [train loss 0.05646], [train acc 0.98250]\n",
      "[epoch 44], [iter 896 / 1121], [train loss 0.05350], [train acc 0.98336]\n",
      "[epoch 44], [iter 1008 / 1121], [train loss 0.05067], [train acc 0.98416]\n",
      "[epoch 44], [iter 1120 / 1121], [train loss 0.05176], [train acc 0.98359]\n",
      "------------------------------------------------------------\n",
      "[epoch 44], [val loss 4.47494], [val acc 0.44153]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 45\n",
      "[epoch 45], [iter 112 / 1121], [train loss 0.04839], [train acc 0.98382]\n",
      "[epoch 45], [iter 224 / 1121], [train loss 0.04782], [train acc 0.98424]\n",
      "[epoch 45], [iter 336 / 1121], [train loss 0.04569], [train acc 0.98512]\n",
      "[epoch 45], [iter 448 / 1121], [train loss 0.04211], [train acc 0.98619]\n",
      "[epoch 45], [iter 560 / 1121], [train loss 0.04356], [train acc 0.98538]\n",
      "[epoch 45], [iter 672 / 1121], [train loss 0.04413], [train acc 0.98507]\n",
      "[epoch 45], [iter 784 / 1121], [train loss 0.04369], [train acc 0.98481]\n",
      "[epoch 45], [iter 896 / 1121], [train loss 0.04542], [train acc 0.98424]\n",
      "[epoch 45], [iter 1008 / 1121], [train loss 0.04750], [train acc 0.98369]\n",
      "[epoch 45], [iter 1120 / 1121], [train loss 0.05005], [train acc 0.98281]\n",
      "------------------------------------------------------------\n",
      "[epoch 45], [val loss 4.20775], [val acc 0.45060]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 46\n",
      "[epoch 46], [iter 112 / 1121], [train loss 0.16717], [train acc 0.94838]\n",
      "[epoch 46], [iter 224 / 1121], [train loss 0.12530], [train acc 0.95982]\n",
      "[epoch 46], [iter 336 / 1121], [train loss 0.10020], [train acc 0.96717]\n",
      "[epoch 46], [iter 448 / 1121], [train loss 0.08830], [train acc 0.97015]\n",
      "[epoch 46], [iter 560 / 1121], [train loss 0.07928], [train acc 0.97344]\n",
      "[epoch 46], [iter 672 / 1121], [train loss 0.07330], [train acc 0.97531]\n",
      "[epoch 46], [iter 784 / 1121], [train loss 0.06708], [train acc 0.97716]\n",
      "[epoch 46], [iter 896 / 1121], [train loss 0.06397], [train acc 0.97824]\n",
      "[epoch 46], [iter 1008 / 1121], [train loss 0.06161], [train acc 0.97904]\n",
      "[epoch 46], [iter 1120 / 1121], [train loss 0.05856], [train acc 0.98019]\n",
      "------------------------------------------------------------\n",
      "[epoch 46], [val loss 4.30571], [val acc 0.45615]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 47\n",
      "[epoch 47], [iter 112 / 1121], [train loss 0.04924], [train acc 0.98214]\n",
      "[epoch 47], [iter 224 / 1121], [train loss 0.03938], [train acc 0.98577]\n",
      "[epoch 47], [iter 336 / 1121], [train loss 0.03765], [train acc 0.98624]\n",
      "[epoch 47], [iter 448 / 1121], [train loss 0.03755], [train acc 0.98668]\n",
      "[epoch 47], [iter 560 / 1121], [train loss 0.03808], [train acc 0.98655]\n",
      "[epoch 47], [iter 672 / 1121], [train loss 0.04139], [train acc 0.98521]\n",
      "[epoch 47], [iter 784 / 1121], [train loss 0.04065], [train acc 0.98565]\n",
      "[epoch 47], [iter 896 / 1121], [train loss 0.04128], [train acc 0.98556]\n",
      "[epoch 47], [iter 1008 / 1121], [train loss 0.04442], [train acc 0.98456]\n",
      "[epoch 47], [iter 1120 / 1121], [train loss 0.04496], [train acc 0.98443]\n",
      "------------------------------------------------------------\n",
      "[epoch 47], [val loss 4.57355], [val acc 0.47631]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 48\n",
      "[epoch 48], [iter 112 / 1121], [train loss 0.10694], [train acc 0.96903]\n",
      "[epoch 48], [iter 224 / 1121], [train loss 0.08858], [train acc 0.97377]\n",
      "[epoch 48], [iter 336 / 1121], [train loss 0.06983], [train acc 0.97861]\n",
      "[epoch 48], [iter 448 / 1121], [train loss 0.06182], [train acc 0.98089]\n",
      "[epoch 48], [iter 560 / 1121], [train loss 0.05853], [train acc 0.98153]\n",
      "[epoch 48], [iter 672 / 1121], [train loss 0.05390], [train acc 0.98247]\n",
      "[epoch 48], [iter 784 / 1121], [train loss 0.04968], [train acc 0.98394]\n",
      "[epoch 48], [iter 896 / 1121], [train loss 0.05020], [train acc 0.98347]\n",
      "[epoch 48], [iter 1008 / 1121], [train loss 0.05018], [train acc 0.98360]\n",
      "[epoch 48], [iter 1120 / 1121], [train loss 0.04969], [train acc 0.98384]\n",
      "------------------------------------------------------------\n",
      "[epoch 48], [val loss 4.27145], [val acc 0.45363]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 49\n",
      "[epoch 49], [iter 112 / 1121], [train loss 0.11488], [train acc 0.95926]\n",
      "[epoch 49], [iter 224 / 1121], [train loss 0.08749], [train acc 0.96777]\n",
      "[epoch 49], [iter 336 / 1121], [train loss 0.07048], [train acc 0.97396]\n",
      "[epoch 49], [iter 448 / 1121], [train loss 0.06020], [train acc 0.97803]\n",
      "[epoch 49], [iter 560 / 1121], [train loss 0.05700], [train acc 0.97924]\n",
      "[epoch 49], [iter 672 / 1121], [train loss 0.05393], [train acc 0.98000]\n",
      "[epoch 49], [iter 784 / 1121], [train loss 0.05140], [train acc 0.98131]\n",
      "[epoch 49], [iter 896 / 1121], [train loss 0.05013], [train acc 0.98162]\n",
      "[epoch 49], [iter 1008 / 1121], [train loss 0.05240], [train acc 0.98109]\n",
      "[epoch 49], [iter 1120 / 1121], [train loss 0.05221], [train acc 0.98131]\n",
      "------------------------------------------------------------\n",
      "[epoch 49], [val loss 4.68042], [val acc 0.45363]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 50\n",
      "[epoch 50], [iter 112 / 1121], [train loss 0.15336], [train acc 0.95033]\n",
      "[epoch 50], [iter 224 / 1121], [train loss 0.10006], [train acc 0.96708]\n",
      "[epoch 50], [iter 336 / 1121], [train loss 0.08065], [train acc 0.97359]\n",
      "[epoch 50], [iter 448 / 1121], [train loss 0.06896], [train acc 0.97691]\n",
      "[epoch 50], [iter 560 / 1121], [train loss 0.06138], [train acc 0.97946]\n",
      "[epoch 50], [iter 672 / 1121], [train loss 0.05632], [train acc 0.98117]\n",
      "[epoch 50], [iter 784 / 1121], [train loss 0.05235], [train acc 0.98210]\n",
      "[epoch 50], [iter 896 / 1121], [train loss 0.04936], [train acc 0.98308]\n",
      "[epoch 50], [iter 1008 / 1121], [train loss 0.04707], [train acc 0.98397]\n",
      "[epoch 50], [iter 1120 / 1121], [train loss 0.04575], [train acc 0.98440]\n",
      "------------------------------------------------------------\n",
      "[epoch 50], [val loss 5.32097], [val acc 0.35534]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "epoch_num = 50\n",
    "# epoch_num = config['hyperparameters']['epochs']\n",
    "print('Start training...  on image size: ', config['preprocessing']['resize'], ' with epoch: ', epoch_num)\n",
    "best_val_acc = 0\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    print('Training on epoch {}'.format(epoch))\n",
    "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    loss_val, acc_val = validate(test_loader, model, criterion, optimizer, epoch)\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        print('*****************************************************')\n",
    "        print('Best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved:  saved_models/224_224_50epoch_0.51acc.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# Check the name of last model in the folder\n",
    "model_path = 'saved_models/'\n",
    "model_name = f'{size}_{size}_{epoch_num}epoch_{best_val_acc:.2f}acc.pth'\n",
    "model_name = os.path.join(model_path, model_name)\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print('Model saved: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(num = 2)\n",
    "fig1 = fig.add_subplot(2,1,1)\n",
    "fig2 = fig.add_subplot(2,1,2)\n",
    "fig1.plot(total_loss_train, label = 'Training loss')\n",
    "fig1.plot(total_acc_train, label = 'Training accuracy')\n",
    "fig2.plot(total_loss_val, label = 'Validation loss')\n",
    "fig2.plot(total_acc_val, label = 'Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
