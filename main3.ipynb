{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Google Colab\n"
     ]
    }
   ],
   "source": [
    "# Google Colab library\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    print(\"In Google Colab\")\n",
    "except:\n",
    "    in_colab = False\n",
    "    print(\"Not in Google Colab\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Python library\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "\n",
    "import yaml\n",
    "import albumentations as A\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Sklearn library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Pytorch library\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# CV2 library\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Cuda version:  12.4\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print(\"Cuda version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name:  Cancer Detection\n",
      "Project version:  DPL302m Project - Fall 2024 of Group 1\n",
      "Project author:  Group 1\n",
      "Project version:  1.0.0\n",
      "Data path:  {'data_path': 'data/', 'raw_path': 'data/raw/', 'raw_csv_path': 'data/raw/HAM10000_metadata.csv', 'raw_images_path': 'data/raw/images/', 'processed_path': 'data/processed/', 'processed_images_path': 'data/processed/images/'}\n",
      "N workers:  12\n",
      "Batch size:  8\n",
      "Preprocessing configuration\n",
      "Augmentation ratios:  1\n",
      "Resize shape:  (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Project information\n",
    "print('Project name: ', config['project']['name'])\n",
    "print('Project version: ', config['project']['description'])\n",
    "print('Project author: ', config['project']['author'])\n",
    "print('Project version: ', config['project']['version'])\n",
    "\n",
    "# Data location\n",
    "print('Data path: ', config['data'])\n",
    "data_path = config['data']['raw_path']\n",
    "csv_path = config['data']['raw_csv_path']\n",
    "images_path = config['data']['raw_images_path']\n",
    "\n",
    "# Training configuration\n",
    "if in_colab:\n",
    "    n_workers = multiprocessing.cpu_count()\n",
    "    batch_size = 128\n",
    "else:\n",
    "    n_workers = config['hyperparameters']['n_workers']\n",
    "    batch_size = config['hyperparameters']['batch_size']\n",
    "print('N workers: ', n_workers)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "# Preprocessing configuration\n",
    "print('Preprocessing configuration')\n",
    "augmentation_ratio = config['preprocessing']['augmentation']['ratio']\n",
    "resize_shape = tuple(config['preprocessing']['resize'])\n",
    "print('Augmentation ratios: ', augmentation_ratio)\n",
    "print('Resize shape: ', resize_shape)\n",
    "\n",
    "# Create a dictionary for images location\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (10015, 7)\n",
      "Data shape: (10015, 7)\n",
      "\n",
      "Data types:\n",
      "lesion_id        object\n",
      "image_id         object\n",
      "dx               object\n",
      "dx_type          object\n",
      "age             float64\n",
      "sex              object\n",
      "localization     object\n",
      "dtype: object\n",
      "\n",
      "Unique values for each column:\n",
      "dx: ['mel' 'nv' 'bkl' 'bcc' 'akiec']...\n",
      "dx_type: ['histo' 'consensus' 'follow_up' 'confocal']...\n",
      "age: [65. 40. 35. 45. 55.]...\n",
      "sex: ['male' 'female' 'unknown']...\n",
      "localization: ['face' 'lower extremity' 'back' 'trunk' 'abdomen']...\n",
      "\n",
      "NaN values in each column:\n",
      "age    57\n",
      "dtype: int64\n",
      "\n",
      "Unknown values in each column:\n",
      "sex: 57\n",
      "localization: 234\n",
      "\n",
      "Summary of potential data issues:\n",
      "- Columns with NaN values: ['age']\n",
      "- Columns with 'unknown' values: ['sex', 'localization']\n",
      "Data after cleaning:\n",
      "        lesion_id                          image_id   dx    dx_type  age  \\\n",
      "1617  HAM_0007180  data/raw/images/ISIC_0033272.jpg  mel      histo   65   \n",
      "8128  HAM_0007195  data/raw/images/ISIC_0031923.jpg   nv      histo   40   \n",
      "2168  HAM_0001835  data/raw/images/ISIC_0026652.jpg  mel      histo   65   \n",
      "1090  HAM_0000465  data/raw/images/ISIC_0030583.jpg  bkl  consensus   35   \n",
      "7754  HAM_0001720  data/raw/images/ISIC_0034010.jpg   nv      histo   45   \n",
      "\n",
      "         sex     localization  dx_code  dx_type_code  sex_code  \n",
      "1617    male             face        4             3         1  \n",
      "8128  female  lower extremity        5             3         0  \n",
      "2168    male             back        4             3         1  \n",
      "1090  female            trunk        2             1         0  \n",
      "7754    male          abdomen        5             3         1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:05<00:00, 166.75it/s]\n",
      "100%|██████████| 245/245 [00:01<00:00, 173.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dx_code  count\n",
      "5        5   5239\n",
      "4        4    858\n",
      "2        2    845\n",
      "1        1    410\n",
      "0        0    256\n",
      "6        6    112\n",
      "3        3     88\n",
      "Number of images: 7808, Number of rows in data: 7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Images: 100%|██████████| 7808/7808 [00:00<00:00, 8502.76it/s] \n",
      "Collecting Results: 100%|██████████| 7808/7808 [00:09<00:00, 859.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented train data shape: (35841, 6)\n",
      "Augmented train images shape: (224, 224, 3)\n",
      "Train data labels shape: (35841,)\n",
      "Raw test data shape: (1953, 6)\n",
      "Saving data...\n",
      "Saving train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  66%|██████▋   | 2977/4481 [00:19<00:09, 151.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m DataProcessor(config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\UNI\\DPL302m\\DPL302m_Project\\scripts\\preprocessing.py:329\u001b[0m, in \u001b[0;36mDataProcessor.run\u001b[1;34m(self, aug, rate)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving train data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_train_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_data(test_data_labels, test_images, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\UNI\\DPL302m\\DPL302m_Project\\scripts\\preprocessing.py:280\u001b[0m, in \u001b[0;36mDataProcessor.save_data\u001b[1;34m(self, data, images, file_suffix)\u001b[0m\n\u001b[0;32m    276\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(augmented_images)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Stack all data into a single tensor\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Compute mean and standard deviation\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m norm_mean, norm_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_img_mean_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m norm_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(norm_mean)\n\u001b[0;32m    282\u001b[0m norm_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(norm_std)\n",
      "File \u001b[1;32md:\\UNI\\DPL302m\\DPL302m_Project\\scripts\\preprocessing.py:248\u001b[0m, in \u001b[0;36mDataProcessor.compute_img_mean_std\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m    246\u001b[0m img_batch \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     img_batch \u001b[38;5;241m=\u001b[39m img_batch\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Compute mean and std for each channel in the batch\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# RGB channels\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scripts.preprocessing import *\n",
    "\n",
    "preprocessor = DataProcessor(config_path='config.yaml')\n",
    "preprocessor.run(aug=True, rate = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu0): ReLU(inplace=True)\n",
      "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (denseblock1): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition(\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition(\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition(\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock(\n",
      "      (denselayer1): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer(\n",
      "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Model building\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    # Variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"Densenet121\":\n",
    "        weights = 'DEFAULT' if use_pretrained else None\n",
    "        model_ft = models.densenet121(weights=weights)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = config['preprocessing']['resize'][0]\n",
    "    \n",
    "    elif model_name == \"Resnet18\":\n",
    "        # Add initialization for Resnet18 if needed\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Step 3: Model training\n",
    "model_name = \"Densenet121\"\n",
    "num_classes = config['hyperparameters']['num_classes']\n",
    "feature_extract = config['hyperparameters']['feature_extract']\n",
    "use_pretrained = config['hyperparameters']['use_pretrained']\n",
    "\n",
    "\n",
    "# Initialize the model for this run and move it to the device\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained)\n",
    "model = model_ft.to(device)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fxlqt\\AppData\\Local\\Temp\\ipykernel_26088\\798468676.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder = torch.load(encoder_file)\n",
      "C:\\Users\\fxlqt\\AppData\\Local\\Temp\\ipykernel_26088\\798468676.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(data_file)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/HAM10000_224x224_train.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m encoder \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(encoder_file)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Load the training data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mHAM10000\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Load the test data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mHAM10000.__init__\u001b[1;34m(self, data_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_file):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load the merged data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\fxlqt\\.conda\\envs\\Py311\\Lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/HAM10000_224x224_train.pt'"
     ]
    }
   ],
   "source": [
    "class HAM10000(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        # Load the merged data\n",
    "        data = torch.load(data_file)\n",
    "        self.images = data['images']\n",
    "        self.labels = data['data']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.images[index]\n",
    "        y = torch.tensor(self.labels[index])\n",
    "        return X, y\n",
    "\n",
    "# Define location of prepared data\n",
    "size = config['preprocessing']['resize'][0]\n",
    "\n",
    "encoder_file = f'encoder/{size}x{size}.pt'\n",
    "train_data_file = f'data/processed/HAM10000_{size}x{size}_train.pt'\n",
    "test_data_file = f'data/processed/HAM10000_{size}x{size}_test.pt'\n",
    "\n",
    "# Load the encoder\n",
    "encoder = torch.load(encoder_file)\n",
    "\n",
    "# Load the training data\n",
    "train_data = HAM10000(train_data_file)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load the test data\n",
    "test_data = HAM10000(test_data_file)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Adam optimizer, use cross entropy loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, report_freq=100):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = outputs.max(1, keepdim=True)[1]\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % report_freq == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "            total_loss_train.append(train_loss.avg)\n",
    "            total_acc_train.append(train_acc.avg)\n",
    "    return train_loss.avg, train_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            prediction = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
    "            val_loss.update(criterion(outputs, labels).item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...  on image size:  [224, 224]  with epoch:  10\n",
      "Training on epoch 1\n",
      "[epoch 1], [iter 100 / 4481], [train loss 2.09069], [train acc 0.12750]\n",
      "[epoch 1], [iter 200 / 4481], [train loss 2.05045], [train acc 0.15188]\n",
      "[epoch 1], [iter 300 / 4481], [train loss 2.04736], [train acc 0.15417]\n",
      "[epoch 1], [iter 400 / 4481], [train loss 2.03962], [train acc 0.15375]\n",
      "[epoch 1], [iter 500 / 4481], [train loss 2.03517], [train acc 0.15525]\n",
      "[epoch 1], [iter 600 / 4481], [train loss 2.03780], [train acc 0.15208]\n",
      "[epoch 1], [iter 700 / 4481], [train loss 2.03753], [train acc 0.14893]\n",
      "[epoch 1], [iter 800 / 4481], [train loss 2.03367], [train acc 0.14875]\n",
      "[epoch 1], [iter 900 / 4481], [train loss 2.03313], [train acc 0.14875]\n",
      "[epoch 1], [iter 1000 / 4481], [train loss 2.02922], [train acc 0.14800]\n",
      "[epoch 1], [iter 1100 / 4481], [train loss 2.02815], [train acc 0.14784]\n",
      "[epoch 1], [iter 1200 / 4481], [train loss 2.02469], [train acc 0.14875]\n",
      "[epoch 1], [iter 1300 / 4481], [train loss 2.02365], [train acc 0.14875]\n",
      "[epoch 1], [iter 1400 / 4481], [train loss 2.02213], [train acc 0.14732]\n",
      "[epoch 1], [iter 1500 / 4481], [train loss 2.02030], [train acc 0.14792]\n",
      "[epoch 1], [iter 1600 / 4481], [train loss 2.01839], [train acc 0.14859]\n",
      "[epoch 1], [iter 1700 / 4481], [train loss 2.01713], [train acc 0.14801]\n",
      "[epoch 1], [iter 1800 / 4481], [train loss 2.01528], [train acc 0.14778]\n",
      "[epoch 1], [iter 1900 / 4481], [train loss 2.01327], [train acc 0.14836]\n",
      "[epoch 1], [iter 2000 / 4481], [train loss 2.01068], [train acc 0.15000]\n",
      "[epoch 1], [iter 2100 / 4481], [train loss 2.00905], [train acc 0.14982]\n",
      "[epoch 1], [iter 2200 / 4481], [train loss 2.00739], [train acc 0.15051]\n",
      "[epoch 1], [iter 2300 / 4481], [train loss 2.00558], [train acc 0.15201]\n",
      "[epoch 1], [iter 2400 / 4481], [train loss 2.00353], [train acc 0.15339]\n",
      "[epoch 1], [iter 2500 / 4481], [train loss 2.00279], [train acc 0.15310]\n",
      "[epoch 1], [iter 2600 / 4481], [train loss 2.00217], [train acc 0.15298]\n",
      "[epoch 1], [iter 2700 / 4481], [train loss 2.00106], [train acc 0.15338]\n",
      "[epoch 1], [iter 2800 / 4481], [train loss 1.99993], [train acc 0.15321]\n",
      "[epoch 1], [iter 2900 / 4481], [train loss 1.99886], [train acc 0.15366]\n",
      "[epoch 1], [iter 3000 / 4481], [train loss 1.99778], [train acc 0.15417]\n",
      "[epoch 1], [iter 3100 / 4481], [train loss 1.99653], [train acc 0.15524]\n",
      "[epoch 1], [iter 3200 / 4481], [train loss 1.99500], [train acc 0.15566]\n",
      "[epoch 1], [iter 3300 / 4481], [train loss 1.99379], [train acc 0.15674]\n",
      "[epoch 1], [iter 3400 / 4481], [train loss 1.99316], [train acc 0.15665]\n",
      "[epoch 1], [iter 3500 / 4481], [train loss 1.99239], [train acc 0.15743]\n",
      "[epoch 1], [iter 3600 / 4481], [train loss 1.99172], [train acc 0.15760]\n",
      "[epoch 1], [iter 3700 / 4481], [train loss 1.99101], [train acc 0.15811]\n",
      "[epoch 1], [iter 3800 / 4481], [train loss 1.99076], [train acc 0.15803]\n",
      "[epoch 1], [iter 3900 / 4481], [train loss 1.99045], [train acc 0.15795]\n",
      "[epoch 1], [iter 4000 / 4481], [train loss 1.98952], [train acc 0.15797]\n",
      "[epoch 1], [iter 4100 / 4481], [train loss 1.98902], [train acc 0.15780]\n",
      "[epoch 1], [iter 4200 / 4481], [train loss 1.98847], [train acc 0.15753]\n",
      "[epoch 1], [iter 4300 / 4481], [train loss 1.98770], [train acc 0.15794]\n",
      "[epoch 1], [iter 4400 / 4481], [train loss 1.98714], [train acc 0.15793]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 2.02321], [val acc 0.22857]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "Best record: [epoch 1], [val loss 2.02321], [val acc 0.22857]\n",
      "*****************************************************\n",
      "Training on epoch 2\n",
      "[epoch 2], [iter 100 / 4481], [train loss 1.95026], [train acc 0.17500]\n",
      "[epoch 2], [iter 200 / 4481], [train loss 1.95395], [train acc 0.16562]\n",
      "[epoch 2], [iter 300 / 4481], [train loss 1.95252], [train acc 0.17125]\n",
      "[epoch 2], [iter 400 / 4481], [train loss 1.95305], [train acc 0.17000]\n",
      "[epoch 2], [iter 500 / 4481], [train loss 1.95304], [train acc 0.17000]\n",
      "[epoch 2], [iter 600 / 4481], [train loss 1.95081], [train acc 0.17396]\n",
      "[epoch 2], [iter 700 / 4481], [train loss 1.95033], [train acc 0.17196]\n",
      "[epoch 2], [iter 800 / 4481], [train loss 1.94791], [train acc 0.17500]\n",
      "[epoch 2], [iter 900 / 4481], [train loss 1.94832], [train acc 0.17653]\n",
      "[epoch 2], [iter 1000 / 4481], [train loss 1.94754], [train acc 0.17788]\n",
      "[epoch 2], [iter 1100 / 4481], [train loss 1.94845], [train acc 0.17648]\n",
      "[epoch 2], [iter 1200 / 4481], [train loss 1.94827], [train acc 0.17563]\n",
      "[epoch 2], [iter 1300 / 4481], [train loss 1.94833], [train acc 0.17452]\n",
      "[epoch 2], [iter 1400 / 4481], [train loss 1.94900], [train acc 0.17259]\n",
      "[epoch 2], [iter 1500 / 4481], [train loss 1.94901], [train acc 0.17292]\n",
      "[epoch 2], [iter 1600 / 4481], [train loss 1.94910], [train acc 0.17219]\n",
      "[epoch 2], [iter 1700 / 4481], [train loss 1.94859], [train acc 0.17169]\n",
      "[epoch 2], [iter 1800 / 4481], [train loss 1.94771], [train acc 0.17326]\n",
      "[epoch 2], [iter 1900 / 4481], [train loss 1.94710], [train acc 0.17342]\n",
      "[epoch 2], [iter 2000 / 4481], [train loss 1.94587], [train acc 0.17425]\n",
      "[epoch 2], [iter 2100 / 4481], [train loss 1.94574], [train acc 0.17464]\n",
      "[epoch 2], [iter 2200 / 4481], [train loss 1.94542], [train acc 0.17528]\n",
      "[epoch 2], [iter 2300 / 4481], [train loss 1.94496], [train acc 0.17516]\n",
      "[epoch 2], [iter 2400 / 4481], [train loss 1.94391], [train acc 0.17677]\n",
      "[epoch 2], [iter 2500 / 4481], [train loss 1.94335], [train acc 0.17685]\n",
      "[epoch 2], [iter 2600 / 4481], [train loss 1.94234], [train acc 0.17793]\n",
      "[epoch 2], [iter 2700 / 4481], [train loss 1.94214], [train acc 0.17875]\n",
      "[epoch 2], [iter 2800 / 4481], [train loss 1.94129], [train acc 0.17964]\n",
      "[epoch 2], [iter 2900 / 4481], [train loss 1.94060], [train acc 0.18039]\n",
      "[epoch 2], [iter 3000 / 4481], [train loss 1.94024], [train acc 0.18067]\n",
      "[epoch 2], [iter 3100 / 4481], [train loss 1.93939], [train acc 0.18145]\n",
      "[epoch 2], [iter 3200 / 4481], [train loss 1.93873], [train acc 0.18227]\n",
      "[epoch 2], [iter 3300 / 4481], [train loss 1.93842], [train acc 0.18246]\n",
      "[epoch 2], [iter 3400 / 4481], [train loss 1.93796], [train acc 0.18254]\n",
      "[epoch 2], [iter 3500 / 4481], [train loss 1.93714], [train acc 0.18304]\n",
      "[epoch 2], [iter 3600 / 4481], [train loss 1.93677], [train acc 0.18365]\n",
      "[epoch 2], [iter 3700 / 4481], [train loss 1.93635], [train acc 0.18416]\n",
      "[epoch 2], [iter 3800 / 4481], [train loss 1.93603], [train acc 0.18388]\n",
      "[epoch 2], [iter 3900 / 4481], [train loss 1.93612], [train acc 0.18394]\n",
      "[epoch 2], [iter 4000 / 4481], [train loss 1.93588], [train acc 0.18438]\n",
      "[epoch 2], [iter 4100 / 4481], [train loss 1.93552], [train acc 0.18549]\n",
      "[epoch 2], [iter 4200 / 4481], [train loss 1.93497], [train acc 0.18565]\n",
      "[epoch 2], [iter 4300 / 4481], [train loss 1.93449], [train acc 0.18599]\n",
      "[epoch 2], [iter 4400 / 4481], [train loss 1.93417], [train acc 0.18614]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 1.73866], [val acc 0.53163]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "Best record: [epoch 2], [val loss 1.73866], [val acc 0.53163]\n",
      "*****************************************************\n",
      "Training on epoch 3\n",
      "[epoch 3], [iter 100 / 4481], [train loss 1.90215], [train acc 0.23375]\n",
      "[epoch 3], [iter 200 / 4481], [train loss 1.90423], [train acc 0.22062]\n",
      "[epoch 3], [iter 300 / 4481], [train loss 1.91378], [train acc 0.20250]\n",
      "[epoch 3], [iter 400 / 4481], [train loss 1.91602], [train acc 0.20187]\n",
      "[epoch 3], [iter 500 / 4481], [train loss 1.91684], [train acc 0.20025]\n",
      "[epoch 3], [iter 600 / 4481], [train loss 1.91669], [train acc 0.20438]\n",
      "[epoch 3], [iter 700 / 4481], [train loss 1.91554], [train acc 0.20518]\n",
      "[epoch 3], [iter 800 / 4481], [train loss 1.91683], [train acc 0.20312]\n",
      "[epoch 3], [iter 900 / 4481], [train loss 1.91485], [train acc 0.20333]\n",
      "[epoch 3], [iter 1000 / 4481], [train loss 1.91328], [train acc 0.20375]\n",
      "[epoch 3], [iter 1100 / 4481], [train loss 1.91194], [train acc 0.20523]\n",
      "[epoch 3], [iter 1200 / 4481], [train loss 1.91042], [train acc 0.20594]\n",
      "[epoch 3], [iter 1300 / 4481], [train loss 1.90976], [train acc 0.20760]\n",
      "[epoch 3], [iter 1400 / 4481], [train loss 1.90833], [train acc 0.20830]\n",
      "[epoch 3], [iter 1500 / 4481], [train loss 1.90827], [train acc 0.20875]\n",
      "[epoch 3], [iter 1600 / 4481], [train loss 1.90831], [train acc 0.20734]\n",
      "[epoch 3], [iter 1700 / 4481], [train loss 1.90835], [train acc 0.20728]\n",
      "[epoch 3], [iter 1800 / 4481], [train loss 1.90790], [train acc 0.20750]\n",
      "[epoch 3], [iter 1900 / 4481], [train loss 1.90680], [train acc 0.20849]\n",
      "[epoch 3], [iter 2000 / 4481], [train loss 1.90553], [train acc 0.20950]\n",
      "[epoch 3], [iter 2100 / 4481], [train loss 1.90459], [train acc 0.21042]\n",
      "[epoch 3], [iter 2200 / 4481], [train loss 1.90352], [train acc 0.21142]\n",
      "[epoch 3], [iter 2300 / 4481], [train loss 1.90261], [train acc 0.21174]\n",
      "[epoch 3], [iter 2400 / 4481], [train loss 1.90136], [train acc 0.21391]\n",
      "[epoch 3], [iter 2500 / 4481], [train loss 1.90046], [train acc 0.21415]\n",
      "[epoch 3], [iter 2600 / 4481], [train loss 1.89853], [train acc 0.21481]\n",
      "[epoch 3], [iter 2700 / 4481], [train loss 1.89794], [train acc 0.21579]\n",
      "[epoch 3], [iter 2800 / 4481], [train loss 1.89680], [train acc 0.21652]\n",
      "[epoch 3], [iter 2900 / 4481], [train loss 1.89634], [train acc 0.21629]\n",
      "[epoch 3], [iter 3000 / 4481], [train loss 1.89572], [train acc 0.21604]\n",
      "[epoch 3], [iter 3100 / 4481], [train loss 1.89549], [train acc 0.21581]\n",
      "[epoch 3], [iter 3200 / 4481], [train loss 1.89533], [train acc 0.21637]\n",
      "[epoch 3], [iter 3300 / 4481], [train loss 1.89497], [train acc 0.21610]\n",
      "[epoch 3], [iter 3400 / 4481], [train loss 1.89445], [train acc 0.21614]\n",
      "[epoch 3], [iter 3500 / 4481], [train loss 1.89370], [train acc 0.21621]\n",
      "[epoch 3], [iter 3600 / 4481], [train loss 1.89285], [train acc 0.21639]\n",
      "[epoch 3], [iter 3700 / 4481], [train loss 1.89204], [train acc 0.21679]\n",
      "[epoch 3], [iter 3800 / 4481], [train loss 1.89128], [train acc 0.21750]\n",
      "[epoch 3], [iter 3900 / 4481], [train loss 1.89078], [train acc 0.21776]\n",
      "[epoch 3], [iter 4000 / 4481], [train loss 1.89026], [train acc 0.21834]\n",
      "[epoch 3], [iter 4100 / 4481], [train loss 1.89039], [train acc 0.21805]\n",
      "[epoch 3], [iter 4200 / 4481], [train loss 1.88968], [train acc 0.21845]\n",
      "[epoch 3], [iter 4300 / 4481], [train loss 1.88929], [train acc 0.21878]\n",
      "[epoch 3], [iter 4400 / 4481], [train loss 1.88845], [train acc 0.21912]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 1.93795], [val acc 0.31684]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 4\n",
      "[epoch 4], [iter 100 / 4481], [train loss 1.86552], [train acc 0.23875]\n",
      "[epoch 4], [iter 200 / 4481], [train loss 1.85925], [train acc 0.24188]\n",
      "[epoch 4], [iter 300 / 4481], [train loss 1.85757], [train acc 0.24125]\n",
      "[epoch 4], [iter 400 / 4481], [train loss 1.86296], [train acc 0.24313]\n",
      "[epoch 4], [iter 500 / 4481], [train loss 1.86141], [train acc 0.24325]\n",
      "[epoch 4], [iter 600 / 4481], [train loss 1.85789], [train acc 0.24500]\n",
      "[epoch 4], [iter 700 / 4481], [train loss 1.85701], [train acc 0.24643]\n",
      "[epoch 4], [iter 800 / 4481], [train loss 1.86088], [train acc 0.24313]\n",
      "[epoch 4], [iter 900 / 4481], [train loss 1.85999], [train acc 0.24278]\n",
      "[epoch 4], [iter 1000 / 4481], [train loss 1.85933], [train acc 0.24088]\n",
      "[epoch 4], [iter 1100 / 4481], [train loss 1.86007], [train acc 0.24102]\n",
      "[epoch 4], [iter 1200 / 4481], [train loss 1.86080], [train acc 0.24156]\n",
      "[epoch 4], [iter 1300 / 4481], [train loss 1.85977], [train acc 0.24048]\n",
      "[epoch 4], [iter 1400 / 4481], [train loss 1.85908], [train acc 0.23848]\n",
      "[epoch 4], [iter 1500 / 4481], [train loss 1.85859], [train acc 0.23950]\n",
      "[epoch 4], [iter 1600 / 4481], [train loss 1.85818], [train acc 0.23953]\n",
      "[epoch 4], [iter 1700 / 4481], [train loss 1.85848], [train acc 0.24074]\n",
      "[epoch 4], [iter 1800 / 4481], [train loss 1.85740], [train acc 0.24153]\n",
      "[epoch 4], [iter 1900 / 4481], [train loss 1.85713], [train acc 0.24184]\n",
      "[epoch 4], [iter 2000 / 4481], [train loss 1.85754], [train acc 0.24119]\n",
      "[epoch 4], [iter 2100 / 4481], [train loss 1.85663], [train acc 0.24179]\n",
      "[epoch 4], [iter 2200 / 4481], [train loss 1.85600], [train acc 0.24205]\n",
      "[epoch 4], [iter 2300 / 4481], [train loss 1.85702], [train acc 0.23984]\n",
      "[epoch 4], [iter 2400 / 4481], [train loss 1.85657], [train acc 0.24010]\n",
      "[epoch 4], [iter 2500 / 4481], [train loss 1.85666], [train acc 0.24035]\n",
      "[epoch 4], [iter 2600 / 4481], [train loss 1.85651], [train acc 0.24072]\n",
      "[epoch 4], [iter 2700 / 4481], [train loss 1.85660], [train acc 0.24056]\n",
      "[epoch 4], [iter 2800 / 4481], [train loss 1.85509], [train acc 0.24170]\n",
      "[epoch 4], [iter 2900 / 4481], [train loss 1.85578], [train acc 0.24151]\n",
      "[epoch 4], [iter 3000 / 4481], [train loss 1.85596], [train acc 0.24158]\n",
      "[epoch 4], [iter 3100 / 4481], [train loss 1.85504], [train acc 0.24286]\n",
      "[epoch 4], [iter 3200 / 4481], [train loss 1.85467], [train acc 0.24348]\n",
      "[epoch 4], [iter 3300 / 4481], [train loss 1.85376], [train acc 0.24390]\n",
      "[epoch 4], [iter 3400 / 4481], [train loss 1.85390], [train acc 0.24415]\n",
      "[epoch 4], [iter 3500 / 4481], [train loss 1.85351], [train acc 0.24446]\n",
      "[epoch 4], [iter 3600 / 4481], [train loss 1.85290], [train acc 0.24455]\n",
      "[epoch 4], [iter 3700 / 4481], [train loss 1.85214], [train acc 0.24520]\n",
      "[epoch 4], [iter 3800 / 4481], [train loss 1.85209], [train acc 0.24553]\n",
      "[epoch 4], [iter 3900 / 4481], [train loss 1.85182], [train acc 0.24551]\n",
      "[epoch 4], [iter 4000 / 4481], [train loss 1.85026], [train acc 0.24631]\n",
      "[epoch 4], [iter 4100 / 4481], [train loss 1.84963], [train acc 0.24616]\n",
      "[epoch 4], [iter 4200 / 4481], [train loss 1.84808], [train acc 0.24690]\n",
      "[epoch 4], [iter 4300 / 4481], [train loss 1.84817], [train acc 0.24715]\n",
      "[epoch 4], [iter 4400 / 4481], [train loss 1.84759], [train acc 0.24736]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 1.98245], [val acc 0.20357]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 5\n",
      "[epoch 5], [iter 100 / 4481], [train loss 1.81660], [train acc 0.26250]\n",
      "[epoch 5], [iter 200 / 4481], [train loss 1.81989], [train acc 0.27000]\n",
      "[epoch 5], [iter 300 / 4481], [train loss 1.81923], [train acc 0.27042]\n",
      "[epoch 5], [iter 400 / 4481], [train loss 1.81894], [train acc 0.26844]\n",
      "[epoch 5], [iter 500 / 4481], [train loss 1.82058], [train acc 0.27000]\n",
      "[epoch 5], [iter 600 / 4481], [train loss 1.81999], [train acc 0.27125]\n",
      "[epoch 5], [iter 700 / 4481], [train loss 1.81927], [train acc 0.26679]\n",
      "[epoch 5], [iter 800 / 4481], [train loss 1.81399], [train acc 0.26875]\n",
      "[epoch 5], [iter 900 / 4481], [train loss 1.81290], [train acc 0.26972]\n",
      "[epoch 5], [iter 1000 / 4481], [train loss 1.81445], [train acc 0.26713]\n",
      "[epoch 5], [iter 1100 / 4481], [train loss 1.81447], [train acc 0.26784]\n",
      "[epoch 5], [iter 1200 / 4481], [train loss 1.81294], [train acc 0.27021]\n",
      "[epoch 5], [iter 1300 / 4481], [train loss 1.81329], [train acc 0.26971]\n",
      "[epoch 5], [iter 1400 / 4481], [train loss 1.81287], [train acc 0.26929]\n",
      "[epoch 5], [iter 1500 / 4481], [train loss 1.81219], [train acc 0.27042]\n",
      "[epoch 5], [iter 1600 / 4481], [train loss 1.81177], [train acc 0.27016]\n",
      "[epoch 5], [iter 1700 / 4481], [train loss 1.81010], [train acc 0.27147]\n",
      "[epoch 5], [iter 1800 / 4481], [train loss 1.80868], [train acc 0.27278]\n",
      "[epoch 5], [iter 1900 / 4481], [train loss 1.80993], [train acc 0.27243]\n",
      "[epoch 5], [iter 2000 / 4481], [train loss 1.80825], [train acc 0.27375]\n",
      "[epoch 5], [iter 2100 / 4481], [train loss 1.80885], [train acc 0.27304]\n",
      "[epoch 5], [iter 2200 / 4481], [train loss 1.80759], [train acc 0.27381]\n",
      "[epoch 5], [iter 2300 / 4481], [train loss 1.80564], [train acc 0.27446]\n",
      "[epoch 5], [iter 2400 / 4481], [train loss 1.80528], [train acc 0.27490]\n",
      "[epoch 5], [iter 2500 / 4481], [train loss 1.80438], [train acc 0.27495]\n",
      "[epoch 5], [iter 2600 / 4481], [train loss 1.80281], [train acc 0.27620]\n",
      "[epoch 5], [iter 2700 / 4481], [train loss 1.80186], [train acc 0.27694]\n",
      "[epoch 5], [iter 2800 / 4481], [train loss 1.80080], [train acc 0.27746]\n",
      "[epoch 5], [iter 2900 / 4481], [train loss 1.79982], [train acc 0.27772]\n",
      "[epoch 5], [iter 3000 / 4481], [train loss 1.79904], [train acc 0.27729]\n",
      "[epoch 5], [iter 3100 / 4481], [train loss 1.79846], [train acc 0.27718]\n",
      "[epoch 5], [iter 3200 / 4481], [train loss 1.79768], [train acc 0.27742]\n",
      "[epoch 5], [iter 3300 / 4481], [train loss 1.79726], [train acc 0.27742]\n",
      "[epoch 5], [iter 3400 / 4481], [train loss 1.79722], [train acc 0.27695]\n",
      "[epoch 5], [iter 3500 / 4481], [train loss 1.79612], [train acc 0.27764]\n",
      "[epoch 5], [iter 3600 / 4481], [train loss 1.79497], [train acc 0.27878]\n",
      "[epoch 5], [iter 3700 / 4481], [train loss 1.79450], [train acc 0.27926]\n",
      "[epoch 5], [iter 3800 / 4481], [train loss 1.79441], [train acc 0.27921]\n",
      "[epoch 5], [iter 3900 / 4481], [train loss 1.79341], [train acc 0.28010]\n",
      "[epoch 5], [iter 4000 / 4481], [train loss 1.79273], [train acc 0.28019]\n",
      "[epoch 5], [iter 4100 / 4481], [train loss 1.79293], [train acc 0.28034]\n",
      "[epoch 5], [iter 4200 / 4481], [train loss 1.79210], [train acc 0.28071]\n",
      "[epoch 5], [iter 4300 / 4481], [train loss 1.79125], [train acc 0.28157]\n",
      "[epoch 5], [iter 4400 / 4481], [train loss 1.79011], [train acc 0.28236]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 2.12119], [val acc 0.11735]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 6\n",
      "[epoch 6], [iter 100 / 4481], [train loss 1.72335], [train acc 0.31500]\n",
      "[epoch 6], [iter 200 / 4481], [train loss 1.73361], [train acc 0.32438]\n",
      "[epoch 6], [iter 300 / 4481], [train loss 1.74088], [train acc 0.31583]\n",
      "[epoch 6], [iter 400 / 4481], [train loss 1.74529], [train acc 0.31125]\n",
      "[epoch 6], [iter 500 / 4481], [train loss 1.74904], [train acc 0.30850]\n",
      "[epoch 6], [iter 600 / 4481], [train loss 1.74026], [train acc 0.31167]\n",
      "[epoch 6], [iter 700 / 4481], [train loss 1.74314], [train acc 0.31232]\n",
      "[epoch 6], [iter 800 / 4481], [train loss 1.74170], [train acc 0.31062]\n",
      "[epoch 6], [iter 900 / 4481], [train loss 1.74407], [train acc 0.31042]\n",
      "[epoch 6], [iter 1000 / 4481], [train loss 1.74363], [train acc 0.31125]\n",
      "[epoch 6], [iter 1100 / 4481], [train loss 1.74067], [train acc 0.31148]\n",
      "[epoch 6], [iter 1200 / 4481], [train loss 1.73811], [train acc 0.31188]\n",
      "[epoch 6], [iter 1300 / 4481], [train loss 1.73446], [train acc 0.31404]\n",
      "[epoch 6], [iter 1400 / 4481], [train loss 1.73388], [train acc 0.31393]\n",
      "[epoch 6], [iter 1500 / 4481], [train loss 1.73376], [train acc 0.31292]\n",
      "[epoch 6], [iter 1600 / 4481], [train loss 1.73368], [train acc 0.31273]\n",
      "[epoch 6], [iter 1700 / 4481], [train loss 1.73229], [train acc 0.31353]\n",
      "[epoch 6], [iter 1800 / 4481], [train loss 1.73135], [train acc 0.31326]\n",
      "[epoch 6], [iter 1900 / 4481], [train loss 1.72898], [train acc 0.31500]\n",
      "[epoch 6], [iter 2000 / 4481], [train loss 1.72822], [train acc 0.31481]\n",
      "[epoch 6], [iter 2100 / 4481], [train loss 1.72701], [train acc 0.31500]\n",
      "[epoch 6], [iter 2200 / 4481], [train loss 1.72653], [train acc 0.31523]\n",
      "[epoch 6], [iter 2300 / 4481], [train loss 1.72591], [train acc 0.31565]\n",
      "[epoch 6], [iter 2400 / 4481], [train loss 1.72475], [train acc 0.31578]\n",
      "[epoch 6], [iter 2500 / 4481], [train loss 1.72267], [train acc 0.31715]\n",
      "[epoch 6], [iter 2600 / 4481], [train loss 1.72283], [train acc 0.31774]\n",
      "[epoch 6], [iter 2700 / 4481], [train loss 1.72059], [train acc 0.31894]\n",
      "[epoch 6], [iter 2800 / 4481], [train loss 1.72020], [train acc 0.31857]\n",
      "[epoch 6], [iter 2900 / 4481], [train loss 1.71897], [train acc 0.31841]\n",
      "[epoch 6], [iter 3000 / 4481], [train loss 1.71776], [train acc 0.31925]\n",
      "[epoch 6], [iter 3100 / 4481], [train loss 1.71588], [train acc 0.32004]\n",
      "[epoch 6], [iter 3200 / 4481], [train loss 1.71495], [train acc 0.32102]\n",
      "[epoch 6], [iter 3300 / 4481], [train loss 1.71368], [train acc 0.32212]\n",
      "[epoch 6], [iter 3400 / 4481], [train loss 1.71288], [train acc 0.32261]\n",
      "[epoch 6], [iter 3500 / 4481], [train loss 1.71126], [train acc 0.32275]\n",
      "[epoch 6], [iter 3600 / 4481], [train loss 1.71091], [train acc 0.32281]\n",
      "[epoch 6], [iter 3700 / 4481], [train loss 1.70917], [train acc 0.32334]\n",
      "[epoch 6], [iter 3800 / 4481], [train loss 1.70827], [train acc 0.32378]\n",
      "[epoch 6], [iter 3900 / 4481], [train loss 1.70671], [train acc 0.32484]\n",
      "[epoch 6], [iter 4000 / 4481], [train loss 1.70540], [train acc 0.32588]\n",
      "[epoch 6], [iter 4100 / 4481], [train loss 1.70475], [train acc 0.32604]\n",
      "[epoch 6], [iter 4200 / 4481], [train loss 1.70338], [train acc 0.32673]\n",
      "[epoch 6], [iter 4300 / 4481], [train loss 1.70174], [train acc 0.32750]\n",
      "[epoch 6], [iter 4400 / 4481], [train loss 1.70161], [train acc 0.32753]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 1.76132], [val acc 0.33316]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 7\n",
      "[epoch 7], [iter 100 / 4481], [train loss 1.64676], [train acc 0.35750]\n",
      "[epoch 7], [iter 200 / 4481], [train loss 1.63725], [train acc 0.36375]\n",
      "[epoch 7], [iter 300 / 4481], [train loss 1.63002], [train acc 0.36458]\n",
      "[epoch 7], [iter 400 / 4481], [train loss 1.63494], [train acc 0.35719]\n",
      "[epoch 7], [iter 500 / 4481], [train loss 1.62810], [train acc 0.36450]\n",
      "[epoch 7], [iter 600 / 4481], [train loss 1.62337], [train acc 0.36938]\n",
      "[epoch 7], [iter 700 / 4481], [train loss 1.62573], [train acc 0.36893]\n",
      "[epoch 7], [iter 800 / 4481], [train loss 1.61903], [train acc 0.36984]\n",
      "[epoch 7], [iter 900 / 4481], [train loss 1.61882], [train acc 0.37014]\n",
      "[epoch 7], [iter 1000 / 4481], [train loss 1.61640], [train acc 0.37188]\n",
      "[epoch 7], [iter 1100 / 4481], [train loss 1.61843], [train acc 0.37080]\n",
      "[epoch 7], [iter 1200 / 4481], [train loss 1.62040], [train acc 0.37000]\n",
      "[epoch 7], [iter 1300 / 4481], [train loss 1.61756], [train acc 0.37173]\n",
      "[epoch 7], [iter 1400 / 4481], [train loss 1.61417], [train acc 0.37411]\n",
      "[epoch 7], [iter 1500 / 4481], [train loss 1.61296], [train acc 0.37333]\n",
      "[epoch 7], [iter 1600 / 4481], [train loss 1.61486], [train acc 0.37227]\n",
      "[epoch 7], [iter 1700 / 4481], [train loss 1.61361], [train acc 0.37265]\n",
      "[epoch 7], [iter 1800 / 4481], [train loss 1.61297], [train acc 0.37215]\n",
      "[epoch 7], [iter 1900 / 4481], [train loss 1.61191], [train acc 0.37224]\n",
      "[epoch 7], [iter 2000 / 4481], [train loss 1.61001], [train acc 0.37344]\n",
      "[epoch 7], [iter 2100 / 4481], [train loss 1.60961], [train acc 0.37405]\n",
      "[epoch 7], [iter 2200 / 4481], [train loss 1.60666], [train acc 0.37557]\n",
      "[epoch 7], [iter 2300 / 4481], [train loss 1.60451], [train acc 0.37668]\n",
      "[epoch 7], [iter 2400 / 4481], [train loss 1.60248], [train acc 0.37693]\n",
      "[epoch 7], [iter 2500 / 4481], [train loss 1.60123], [train acc 0.37680]\n",
      "[epoch 7], [iter 2600 / 4481], [train loss 1.60057], [train acc 0.37683]\n",
      "[epoch 7], [iter 2700 / 4481], [train loss 1.60039], [train acc 0.37662]\n",
      "[epoch 7], [iter 2800 / 4481], [train loss 1.59915], [train acc 0.37638]\n",
      "[epoch 7], [iter 2900 / 4481], [train loss 1.59757], [train acc 0.37642]\n",
      "[epoch 7], [iter 3000 / 4481], [train loss 1.59673], [train acc 0.37688]\n",
      "[epoch 7], [iter 3100 / 4481], [train loss 1.59472], [train acc 0.37778]\n",
      "[epoch 7], [iter 3200 / 4481], [train loss 1.59359], [train acc 0.37840]\n",
      "[epoch 7], [iter 3300 / 4481], [train loss 1.59409], [train acc 0.37750]\n",
      "[epoch 7], [iter 3400 / 4481], [train loss 1.59171], [train acc 0.37871]\n",
      "[epoch 7], [iter 3500 / 4481], [train loss 1.59066], [train acc 0.37918]\n",
      "[epoch 7], [iter 3600 / 4481], [train loss 1.58857], [train acc 0.38003]\n",
      "[epoch 7], [iter 3700 / 4481], [train loss 1.58648], [train acc 0.38118]\n",
      "[epoch 7], [iter 3800 / 4481], [train loss 1.58563], [train acc 0.38109]\n",
      "[epoch 7], [iter 3900 / 4481], [train loss 1.58494], [train acc 0.38109]\n",
      "[epoch 7], [iter 4000 / 4481], [train loss 1.58436], [train acc 0.38122]\n",
      "[epoch 7], [iter 4100 / 4481], [train loss 1.58397], [train acc 0.38088]\n",
      "[epoch 7], [iter 4200 / 4481], [train loss 1.58245], [train acc 0.38143]\n",
      "[epoch 7], [iter 4300 / 4481], [train loss 1.58221], [train acc 0.38140]\n",
      "[epoch 7], [iter 4400 / 4481], [train loss 1.58044], [train acc 0.38222]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 1.79440], [val acc 0.30612]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 8\n",
      "[epoch 8], [iter 100 / 4481], [train loss 1.53762], [train acc 0.38875]\n",
      "[epoch 8], [iter 200 / 4481], [train loss 1.49869], [train acc 0.40875]\n",
      "[epoch 8], [iter 300 / 4481], [train loss 1.51322], [train acc 0.40333]\n",
      "[epoch 8], [iter 400 / 4481], [train loss 1.50924], [train acc 0.40531]\n",
      "[epoch 8], [iter 500 / 4481], [train loss 1.51143], [train acc 0.40675]\n",
      "[epoch 8], [iter 600 / 4481], [train loss 1.50089], [train acc 0.41125]\n",
      "[epoch 8], [iter 700 / 4481], [train loss 1.50305], [train acc 0.41125]\n",
      "[epoch 8], [iter 800 / 4481], [train loss 1.50483], [train acc 0.40687]\n",
      "[epoch 8], [iter 900 / 4481], [train loss 1.50154], [train acc 0.41028]\n",
      "[epoch 8], [iter 1000 / 4481], [train loss 1.50423], [train acc 0.40713]\n",
      "[epoch 8], [iter 1100 / 4481], [train loss 1.50191], [train acc 0.40852]\n",
      "[epoch 8], [iter 1200 / 4481], [train loss 1.49832], [train acc 0.41094]\n",
      "[epoch 8], [iter 1300 / 4481], [train loss 1.49619], [train acc 0.41096]\n",
      "[epoch 8], [iter 1400 / 4481], [train loss 1.50009], [train acc 0.40920]\n",
      "[epoch 8], [iter 1500 / 4481], [train loss 1.50033], [train acc 0.40967]\n",
      "[epoch 8], [iter 1600 / 4481], [train loss 1.49541], [train acc 0.41187]\n",
      "[epoch 8], [iter 1700 / 4481], [train loss 1.49894], [train acc 0.40971]\n",
      "[epoch 8], [iter 1800 / 4481], [train loss 1.50130], [train acc 0.40937]\n",
      "[epoch 8], [iter 1900 / 4481], [train loss 1.50279], [train acc 0.40882]\n",
      "[epoch 8], [iter 2000 / 4481], [train loss 1.49995], [train acc 0.41025]\n",
      "[epoch 8], [iter 2100 / 4481], [train loss 1.50037], [train acc 0.40988]\n",
      "[epoch 8], [iter 2200 / 4481], [train loss 1.49747], [train acc 0.41170]\n",
      "[epoch 8], [iter 2300 / 4481], [train loss 1.49751], [train acc 0.41239]\n",
      "[epoch 8], [iter 2400 / 4481], [train loss 1.49704], [train acc 0.41297]\n",
      "[epoch 8], [iter 2500 / 4481], [train loss 1.49589], [train acc 0.41415]\n",
      "[epoch 8], [iter 2600 / 4481], [train loss 1.49422], [train acc 0.41457]\n",
      "[epoch 8], [iter 2700 / 4481], [train loss 1.49208], [train acc 0.41514]\n",
      "[epoch 8], [iter 2800 / 4481], [train loss 1.49094], [train acc 0.41549]\n",
      "[epoch 8], [iter 2900 / 4481], [train loss 1.48979], [train acc 0.41638]\n",
      "[epoch 8], [iter 3000 / 4481], [train loss 1.48930], [train acc 0.41750]\n",
      "[epoch 8], [iter 3100 / 4481], [train loss 1.48848], [train acc 0.41774]\n",
      "[epoch 8], [iter 3200 / 4481], [train loss 1.48790], [train acc 0.41758]\n",
      "[epoch 8], [iter 3300 / 4481], [train loss 1.48734], [train acc 0.41780]\n",
      "[epoch 8], [iter 3400 / 4481], [train loss 1.48674], [train acc 0.41798]\n",
      "[epoch 8], [iter 3500 / 4481], [train loss 1.48345], [train acc 0.41957]\n",
      "[epoch 8], [iter 3600 / 4481], [train loss 1.48110], [train acc 0.42056]\n",
      "[epoch 8], [iter 3700 / 4481], [train loss 1.47992], [train acc 0.42115]\n",
      "[epoch 8], [iter 3800 / 4481], [train loss 1.47845], [train acc 0.42151]\n",
      "[epoch 8], [iter 3900 / 4481], [train loss 1.47694], [train acc 0.42173]\n",
      "[epoch 8], [iter 4000 / 4481], [train loss 1.47579], [train acc 0.42206]\n",
      "[epoch 8], [iter 4100 / 4481], [train loss 1.47365], [train acc 0.42299]\n",
      "[epoch 8], [iter 4200 / 4481], [train loss 1.47324], [train acc 0.42324]\n",
      "[epoch 8], [iter 4300 / 4481], [train loss 1.47307], [train acc 0.42317]\n",
      "[epoch 8], [iter 4400 / 4481], [train loss 1.47069], [train acc 0.42409]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 2.05428], [val acc 0.21990]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 9\n",
      "[epoch 9], [iter 100 / 4481], [train loss 1.39175], [train acc 0.45875]\n",
      "[epoch 9], [iter 200 / 4481], [train loss 1.39688], [train acc 0.47437]\n",
      "[epoch 9], [iter 300 / 4481], [train loss 1.38438], [train acc 0.47542]\n",
      "[epoch 9], [iter 400 / 4481], [train loss 1.37475], [train acc 0.47656]\n",
      "[epoch 9], [iter 500 / 4481], [train loss 1.38759], [train acc 0.46775]\n",
      "[epoch 9], [iter 600 / 4481], [train loss 1.38468], [train acc 0.46333]\n",
      "[epoch 9], [iter 700 / 4481], [train loss 1.38652], [train acc 0.46357]\n",
      "[epoch 9], [iter 800 / 4481], [train loss 1.38746], [train acc 0.46219]\n",
      "[epoch 9], [iter 900 / 4481], [train loss 1.38778], [train acc 0.46194]\n",
      "[epoch 9], [iter 1000 / 4481], [train loss 1.38836], [train acc 0.46313]\n",
      "[epoch 9], [iter 1100 / 4481], [train loss 1.38708], [train acc 0.46477]\n",
      "[epoch 9], [iter 1200 / 4481], [train loss 1.39115], [train acc 0.46406]\n",
      "[epoch 9], [iter 1300 / 4481], [train loss 1.38858], [train acc 0.46413]\n",
      "[epoch 9], [iter 1400 / 4481], [train loss 1.38761], [train acc 0.46357]\n",
      "[epoch 9], [iter 1500 / 4481], [train loss 1.38603], [train acc 0.46242]\n",
      "[epoch 9], [iter 1600 / 4481], [train loss 1.38438], [train acc 0.46305]\n",
      "[epoch 9], [iter 1700 / 4481], [train loss 1.38134], [train acc 0.46338]\n",
      "[epoch 9], [iter 1800 / 4481], [train loss 1.37915], [train acc 0.46347]\n",
      "[epoch 9], [iter 1900 / 4481], [train loss 1.37587], [train acc 0.46507]\n",
      "[epoch 9], [iter 2000 / 4481], [train loss 1.37568], [train acc 0.46481]\n",
      "[epoch 9], [iter 2100 / 4481], [train loss 1.37470], [train acc 0.46601]\n",
      "[epoch 9], [iter 2200 / 4481], [train loss 1.37286], [train acc 0.46705]\n",
      "[epoch 9], [iter 2300 / 4481], [train loss 1.37485], [train acc 0.46560]\n",
      "[epoch 9], [iter 2400 / 4481], [train loss 1.37353], [train acc 0.46708]\n",
      "[epoch 9], [iter 2500 / 4481], [train loss 1.37199], [train acc 0.46700]\n",
      "[epoch 9], [iter 2600 / 4481], [train loss 1.37172], [train acc 0.46760]\n",
      "[epoch 9], [iter 2700 / 4481], [train loss 1.37073], [train acc 0.46819]\n",
      "[epoch 9], [iter 2800 / 4481], [train loss 1.37002], [train acc 0.46821]\n",
      "[epoch 9], [iter 2900 / 4481], [train loss 1.36922], [train acc 0.46836]\n",
      "[epoch 9], [iter 3000 / 4481], [train loss 1.36817], [train acc 0.46896]\n",
      "[epoch 9], [iter 3100 / 4481], [train loss 1.36798], [train acc 0.46915]\n",
      "[epoch 9], [iter 3200 / 4481], [train loss 1.36743], [train acc 0.46926]\n",
      "[epoch 9], [iter 3300 / 4481], [train loss 1.36653], [train acc 0.46985]\n",
      "[epoch 9], [iter 3400 / 4481], [train loss 1.36624], [train acc 0.47029]\n",
      "[epoch 9], [iter 3500 / 4481], [train loss 1.36493], [train acc 0.47171]\n",
      "[epoch 9], [iter 3600 / 4481], [train loss 1.36496], [train acc 0.47240]\n",
      "[epoch 9], [iter 3700 / 4481], [train loss 1.36532], [train acc 0.47189]\n",
      "[epoch 9], [iter 3800 / 4481], [train loss 1.36552], [train acc 0.47184]\n",
      "[epoch 9], [iter 3900 / 4481], [train loss 1.36520], [train acc 0.47115]\n",
      "[epoch 9], [iter 4000 / 4481], [train loss 1.36618], [train acc 0.47041]\n",
      "[epoch 9], [iter 4100 / 4481], [train loss 1.36622], [train acc 0.47049]\n",
      "[epoch 9], [iter 4200 / 4481], [train loss 1.36533], [train acc 0.47092]\n",
      "[epoch 9], [iter 4300 / 4481], [train loss 1.36394], [train acc 0.47180]\n",
      "[epoch 9], [iter 4400 / 4481], [train loss 1.36321], [train acc 0.47207]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 1.98885], [val acc 0.23980]\n",
      "------------------------------------------------------------\n",
      "Training on epoch 10\n",
      "[epoch 10], [iter 100 / 4481], [train loss 1.24797], [train acc 0.51875]\n",
      "[epoch 10], [iter 200 / 4481], [train loss 1.26090], [train acc 0.51812]\n",
      "[epoch 10], [iter 300 / 4481], [train loss 1.27696], [train acc 0.51292]\n",
      "[epoch 10], [iter 400 / 4481], [train loss 1.27341], [train acc 0.51219]\n",
      "[epoch 10], [iter 500 / 4481], [train loss 1.28734], [train acc 0.50750]\n",
      "[epoch 10], [iter 600 / 4481], [train loss 1.29259], [train acc 0.50708]\n",
      "[epoch 10], [iter 700 / 4481], [train loss 1.28470], [train acc 0.51036]\n",
      "[epoch 10], [iter 800 / 4481], [train loss 1.27967], [train acc 0.51187]\n",
      "[epoch 10], [iter 900 / 4481], [train loss 1.27829], [train acc 0.51278]\n",
      "[epoch 10], [iter 1000 / 4481], [train loss 1.27660], [train acc 0.51175]\n",
      "[epoch 10], [iter 1100 / 4481], [train loss 1.28309], [train acc 0.50852]\n",
      "[epoch 10], [iter 1200 / 4481], [train loss 1.28505], [train acc 0.50646]\n",
      "[epoch 10], [iter 1300 / 4481], [train loss 1.28531], [train acc 0.50481]\n",
      "[epoch 10], [iter 1400 / 4481], [train loss 1.28077], [train acc 0.50687]\n",
      "[epoch 10], [iter 1500 / 4481], [train loss 1.28293], [train acc 0.50517]\n",
      "[epoch 10], [iter 1600 / 4481], [train loss 1.28005], [train acc 0.50602]\n",
      "[epoch 10], [iter 1700 / 4481], [train loss 1.28174], [train acc 0.50500]\n",
      "[epoch 10], [iter 1800 / 4481], [train loss 1.28053], [train acc 0.50556]\n",
      "[epoch 10], [iter 1900 / 4481], [train loss 1.28147], [train acc 0.50513]\n",
      "[epoch 10], [iter 2000 / 4481], [train loss 1.27908], [train acc 0.50650]\n",
      "[epoch 10], [iter 2100 / 4481], [train loss 1.27946], [train acc 0.50577]\n",
      "[epoch 10], [iter 2200 / 4481], [train loss 1.27906], [train acc 0.50591]\n",
      "[epoch 10], [iter 2300 / 4481], [train loss 1.27600], [train acc 0.50772]\n",
      "[epoch 10], [iter 2400 / 4481], [train loss 1.27525], [train acc 0.50766]\n",
      "[epoch 10], [iter 2500 / 4481], [train loss 1.27558], [train acc 0.50805]\n",
      "[epoch 10], [iter 2600 / 4481], [train loss 1.27550], [train acc 0.50798]\n",
      "[epoch 10], [iter 2700 / 4481], [train loss 1.27328], [train acc 0.50843]\n",
      "[epoch 10], [iter 2800 / 4481], [train loss 1.27205], [train acc 0.50853]\n",
      "[epoch 10], [iter 2900 / 4481], [train loss 1.27129], [train acc 0.50922]\n",
      "[epoch 10], [iter 3000 / 4481], [train loss 1.27112], [train acc 0.50800]\n",
      "[epoch 10], [iter 3100 / 4481], [train loss 1.27049], [train acc 0.50819]\n",
      "[epoch 10], [iter 3200 / 4481], [train loss 1.27079], [train acc 0.50762]\n",
      "[epoch 10], [iter 3300 / 4481], [train loss 1.26926], [train acc 0.50807]\n",
      "[epoch 10], [iter 3400 / 4481], [train loss 1.26994], [train acc 0.50765]\n",
      "[epoch 10], [iter 3500 / 4481], [train loss 1.27118], [train acc 0.50746]\n",
      "[epoch 10], [iter 3600 / 4481], [train loss 1.27084], [train acc 0.50781]\n",
      "[epoch 10], [iter 3700 / 4481], [train loss 1.26920], [train acc 0.50814]\n",
      "[epoch 10], [iter 3800 / 4481], [train loss 1.26820], [train acc 0.50855]\n",
      "[epoch 10], [iter 3900 / 4481], [train loss 1.26702], [train acc 0.50920]\n",
      "[epoch 10], [iter 4000 / 4481], [train loss 1.26753], [train acc 0.50894]\n",
      "[epoch 10], [iter 4100 / 4481], [train loss 1.26625], [train acc 0.51000]\n",
      "[epoch 10], [iter 4200 / 4481], [train loss 1.26601], [train acc 0.50967]\n",
      "[epoch 10], [iter 4300 / 4481], [train loss 1.26541], [train acc 0.51026]\n",
      "[epoch 10], [iter 4400 / 4481], [train loss 1.26410], [train acc 0.51057]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 1.74138], [val acc 0.33776]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_loss_train, total_acc_train = [],[]\n",
    "epoch_num = 10\n",
    "# epoch_num = config['hyperparameters']['epochs']\n",
    "print('Start training...  on image size: ', config['preprocessing']['resize'], ' with epoch: ', epoch_num)\n",
    "best_val_acc = 0\n",
    "total_loss_val, total_acc_val = [],[]\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    print('Training on epoch {}'.format(epoch))\n",
    "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch, report_freq=100)\n",
    "    loss_val, acc_val = validate(test_loader, model, criterion, optimizer, epoch)\n",
    "    total_loss_val.append(loss_val)\n",
    "    total_acc_val.append(acc_val)\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        print('*****************************************************')\n",
    "        print('Best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved:  saved_models/224_224_10epoch_0.53acc.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# Check the name of last model in the folder\n",
    "model_path = 'saved_models/'\n",
    "model_name = f'{size}_{size}_{epoch_num}epoch_{best_val_acc:.2f}acc.pth'\n",
    "model_name = os.path.join(model_path, model_name)\n",
    "torch.save(model.state_dict(), model_name)\n",
    "print('Model saved: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(num = 2)\n",
    "fig1 = fig.add_subplot(2,1,1)\n",
    "fig2 = fig.add_subplot(2,1,2)\n",
    "fig1.plot(total_loss_train, label = 'Training loss')\n",
    "fig1.plot(total_acc_train, label = 'Training accuracy')\n",
    "fig2.plot(total_loss_val, label = 'Validation loss')\n",
    "fig2.plot(total_acc_val, label = 'Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
